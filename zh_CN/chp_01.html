
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>第一章: 贝叶斯推断 — Bayesian Modeling and Computation in Python</title>
<link href="../_static/css/theme.css" rel="stylesheet"/>
<link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" rel="stylesheet" type="text/css">
<link href="../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-codeautolink.css" rel="stylesheet" type="text/css"/>
<link href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js" rel="preload"/>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/togglebutton.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../_static/sphinx-thebe.js"></script>
<link href="../_static/favicon.ico" rel="shortcut icon">
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="chp_02.html" rel="next" title="第二章: 贝叶斯模型的探索性分析"/>
<link href="symbollist.html" rel="prev" title="符号表"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>
</link></link></link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
<h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
</a>
</div><form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="current nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="dedication.html">
   贡献
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="foreword.html">
   序言
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="preface.html">
   前言
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="symbollist.html">
   符号表
  </a>
</li>
<li class="toctree-l1 current active">
<a class="current reference internal" href="#">
   第一章: 贝叶斯推断
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_02.html">
   第二章: 贝叶斯模型的探索性分析
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_03.html">
   第三章：线性模型与概率编程语言
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_04.html">
   第四章：扩展线性模型
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_05.html">
   第五章: 样条
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_06.html">
   第六章: 时间序列
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_07.html">
   第七章：贝叶斯加性回归树
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_08.html">
   第八章：近似贝叶斯计算
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_09.html">
   第九章: 端到端的贝叶斯工作流
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_10.html">
   第十章: 概率编程语言
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="chp_11.html">
   第十一章: 附加主题
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="glossary.html">
   词汇表
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="references.html">
   References
  </a>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fzh_CN/chp_01.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bayesian-modeling">
   1.1 贝叶斯建模
  </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bayesian-models">
     1.1.1 贝叶斯模型及其工作流程
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bayesian-inference">
     1.1.2 贝叶斯推断
    </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#sampling-methods-intro">
   1.2 一个自制的采样器
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#automating-inference">
   1.3 人工建模与自动推断
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-prior-count">
   1.4 量化先验信息的几种选择
  </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#conjugate-priors">
     1.4.1 共轭先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#objective-priors">
     1.4.2 客观先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#maximum-entropy-priors">
     1.4.3 最大熵先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#weakly-informative-priors-and-regularization-priors">
     1.4.4 弱信息性先验和正则化先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#using-prior-predictive-distributions-to-assess-priors">
     1.4.5 使用先验预测分布评估先验
    </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercises1">
   1.5 练习
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>第一章: 贝叶斯推断</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bayesian-modeling">
   1.1 贝叶斯建模
  </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bayesian-models">
     1.1.1 贝叶斯模型及其工作流程
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bayesian-inference">
     1.1.2 贝叶斯推断
    </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#sampling-methods-intro">
   1.2 一个自制的采样器
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#automating-inference">
   1.3 人工建模与自动推断
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-prior-count">
   1.4 量化先验信息的几种选择
  </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#conjugate-priors">
     1.4.1 共轭先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#objective-priors">
     1.4.2 客观先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#maximum-entropy-priors">
     1.4.3 最大熵先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#weakly-informative-priors-and-regularization-priors">
     1.4.4 弱信息性先验和正则化先验
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#using-prior-predictive-distributions-to-assess-priors">
     1.4.5 使用先验预测分布评估先验
    </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercises1">
   1.5 练习
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div>
<div class="tex2jax_ignore mathjax_ignore section" id="chap1">
<span id="id1"></span><h1>第一章: 贝叶斯推断<a class="headerlink" href="#chap1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>现代贝叶斯统计主要使用计算机代码执行，这极大地改变了贝叶斯统计的执行方式。我们能够建立的模型的复杂性越来越大，必要的数学和计算技能障碍逐步降低。此外，迭代式建模过程也变得比以往更容易执行和更有价值。计算机方法的普及和流行确实很棒，但也需要更高的责任感。现在表达统计方法比以往任何时候都容易，但统计是一个微妙的领域，强大的计算方法并不会使统计神奇地消失。因此，具有良好理论背景知识（尤其是那些与实践相关的知识），对于有效应用统计方法非常重要。在本章中，我们介绍了其中一些基础概念和方法，还有很多内容将在本书其余部分进一步探索和扩展。</p>
<div class="section" id="bayesian-modeling">
<span id="id2"></span><h2>1.1 贝叶斯建模<a class="headerlink" href="#bayesian-modeling" title="Permalink to this headline">¶</a></h2>
<p>概念模型是对一个系统的表征，它由若干概念组合而成，用于帮助人们了解、理解或者模拟该模型所代表的对象或过程 <span id="id3">[<a class="reference internal" href="references.html#id25">6</a>]</span>。此外，模型是人为设计的表示，具有非常具体的目标，因此，讨论模型对给定问题的充分性通常比讨论其内在正确性更方便。模型的存在仅仅是为了帮助实现进一步的目标。</p>
<p>在设计新车时，汽车公司会制作一个物理模型，以帮助人们理解产品在制造时的外观。此时，一位具有汽车先验知识并且对如何使用模型具有很好估计的雕刻家，会寻找所需粘土等原材料，并使用手工工具雕刻物理模型。此物理模型能够帮助其他人了解设计的各个方面，例如：外观是否美观、汽车形状是否符合空气动力学等。这样的模型需要同时结合领域专业知识和雕刻知识才能得到想要的结果。此外，建模过程通常需要构建多个模型，这样做有可能是为了探索不同的选择，或者是因为需要与其他汽车设计团队成员互动来获得迭代式改进和扩展。如今，除了上述实体汽车模型外，通过计算机辅助设计软件制作的数字模型也很常见。</p>
<p>这种计算机模型相比物理模型存在一些优势。例如：与在实体汽车模型上进行测试相比，使用数字模型进行碰撞模拟更简单、更便宜；与团队内的同事共享模型也更加容易。</p>
<p>贝叶斯建模的思想与上述汽车建模非常相似。构建一个贝叶斯模型，需要结合领域专业知识和统计技能，以将知识整合到一些可计算的目标中，并确定结果的可用性。在贝叶斯建模场景中，<em>数据</em> 是“原材料”，而 <em>统计分布</em> 则是塑造模型的主要数学工具。需要同时结合领域专业知识和统计知识才能获得有用的结果。此外，贝叶斯实践者同样会以迭代方式构建多个模型，其中第一个基础模型主要用于帮助自身识别与思维的差距或模型存在的缺陷，然后将这些模型用于构建后续的改进模型和扩展模型。</p>
<p>此外，使用一种推断机制并不意味着阻碍其他推断机制发挥作用，就像汽车的物理模型不会阻碍数字模型发挥效用一样。同样的，现代贝叶斯实践者也有很多方式来表达想法、生成结果和分享输出，从而允许实践者及同行能够更广泛地推广其积极成果。</p>
<div class="section" id="bayesian-models">
<span id="id4"></span><h3>1.1.1 贝叶斯模型及其工作流程<a class="headerlink" href="#bayesian-models" title="Permalink to this headline">¶</a></h3>
<p>贝叶斯模型，无论是可计算模型还是其他模型，都有两个基本定义特征：</p>
<ul class="simple">
<li><p>一是使用概率分布描述未知量 <a class="footnote-reference brackets" href="#id44" id="id5">1</a> ，我们称这些未知量为参数 <a class="footnote-reference brackets" href="#id45" id="id6">2</a>。</p></li>
<li><p>二是采用贝叶斯定理，以数据为条件更新参数的值，此过程也可以被视为概率的重新分配。</p></li>
</ul>
<p>在高层次上，可以将贝叶斯模型的构建过程分为三个步骤：</p>
<ol class="simple">
<li><p><strong>模型设计</strong>：给定一些数据,以及关于如何生成这些数据的一些假设，我们通过 <em>组合（ Combing ）</em> 和 <em>转换（ Transforming ）</em> 随机变量来设计模型。</p></li>
<li><p><strong>模型推断</strong>：利用贝叶斯定理，使所设计的模型能够适应数据。我们称此过程为<em>推断（ Inference ）</em>，其结果是获得了后验分布。我们希望数据能够减少所有可能参数值的不确定性，尽管这并不是任何贝叶斯模型都能保证的。</p></li>
<li><p><strong>模型评判</strong>：我们根据不同标准来检查模型是否有意义，进而对模型进行评判，这些标准包括数据以及我们的专业领域知识等。模型来自于我们的设计，因此其自身天然具有不确定性，通过比较多个模型，在理论上会减少模型的不确定性。</p></li>
</ol>
<p>如果你熟悉其他形式的建模工作，你就会认识到模型评判的重要性，以及迭代式地执行此三个步骤的必要性。例如：我们可能需要在任何给定点回溯历史步骤。这也许是因为我们引入了一个愚蠢的编程错误，或者在一些挑战之后我们找到了改进模型的方法，或者我们发现数据不像最初想象的那样可用，以至于我们需要收集更多数据甚至是不同类型的数据。</p>
<p>在本书中，我们将讨论这三个步骤中每一步的各种执行方法，并将学习如何将其扩展为更复杂的<strong>贝叶斯工作流（ Bayesian Workflow ）</strong>。我们认为“贝叶斯工作流”非常重要，以至于本书用了完整的一章（ <a class="reference internal" href="chp_09.html#chap9"><span class="std std-ref">第 9 章</span></a> ）来审视和讨论此主题。</p>
</div>
<div class="section" id="bayesian-inference">
<span id="id7"></span><h3>1.1.2 贝叶斯推断<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline">¶</a></h3>
<p>通俗地说，推断与 “根据证据和推理得出结论” 有关。贝叶斯推断是一种特殊形式的统计推断，它通过组合概率分布来获得其他概率分布。当我们已经观测到一些数据 <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span> 时，贝叶斯定理提供了用于估计参数 <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> 值的通用方法：</p>
<div class="math notranslate nohighlight" id="equation-eq-posterior-dist">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-posterior-dist" title="Permalink to this equation">¶</a></span>\[\underbrace{p(\boldsymbol{\theta} \mid \boldsymbol{Y})}_{\text{posterior}} = \frac{\overbrace{p(\boldsymbol{Y} \mid \boldsymbol{\theta})}^{\text{likelihood}}\; \overbrace{p(\boldsymbol{\theta})}^{\text{prior}}}{\underbrace{{p(\boldsymbol{Y})}}_{\text{marginal likelihood}}}\]</div>
<p>似然函数将观测数据与未知参数链接起来，而先验分布表示在观测到数据 <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span> 之前的参数不确定性 <a class="footnote-reference brackets" href="#id46" id="id8">3</a>。通过将两者相乘，可以得到后验分布，即模型中所有未知参数的联合分布（ 以观测数据为条件 ）。</p>
<p><a class="reference internal" href="#fig-bayesian-triad"><span class="std std-numref">Fig. 1</span></a> 展示了一个任意的先验分布、似然函数，以及两者产生的后验分布 <a class="footnote-reference brackets" href="#id47" id="id9">4</a> 。</p>
<div class="figure align-default" id="fig-bayesian-triad">
<a class="reference internal image-reference" href="../_images/bayesian_triad.png"><img alt="../_images/bayesian_triad.png" src="../_images/bayesian_triad.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">左子图：一个假想的先验（黑色曲线）表明， <span class="math notranslate nohighlight">\(\theta = 0.5\)</span> 的可能性更大，而其余值则呈现出线性对称的下降趋势；似然函数（灰色曲线）则表明， <span class="math notranslate nohighlight">\(\theta = 0.2\)</span> 的值能更好地解释数据；而结果后验（蓝色曲线）先验和似然之间的折衷。图中省略了 <span class="math notranslate nohighlight">\(y\)</span> 轴的值，因为我们只关心相对值。右子图：其功能与左子图相同，但 <span class="math notranslate nohighlight">\(y\)</span> 轴采用了对数尺度。请注意，对数尺度能够保留相对性质，例如，两个子图中最大值和最小值所在位置并没有改变。对数尺度由于数值计算更稳定而成为计算机实现的首选。</span><a class="headerlink" href="#fig-bayesian-triad" title="Permalink to this image">¶</a></p>
</div>
<p>请注意，虽然 <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span> 是观测数据，但它也被视为一个随机向量，因为它的值取决于特定实验结果 <a class="footnote-reference brackets" href="#id48" id="id10">5</a>。为了获得后验分布，我们会将该随机向量的值视为固定在实际观测值上不变，出于此原因，一个常见的替代符号是使用 <span class="math notranslate nohighlight">\(y_{obs}\)</span> ，而不是 <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span>。</p>
<p>正如你看到的，在每个特定点上评估后验，在概念上非常简单，我们只需将 <em>先验</em> 乘以 <em>似然</em> 即可。然而，这不足以告诉我们后验概率的全部，因为我们不仅需要特定点处的绝对后验值，还需要其与周围点的相对值。后验分布的这种全局信息由归一化常数来表示，而且不幸的是，计算归一化常数 <span class="math notranslate nohighlight">\(p(\boldsymbol{Y})\)</span> 非常困难。将边缘似然写成如下公式可能更容易理解这一点：</p>
<div class="math notranslate nohighlight" id="equation-eq-marginal-likelihood">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-marginal-likelihood" title="Permalink to this equation">¶</a></span>\[{p(\boldsymbol{Y}) = \int_{\boldsymbol{\Theta}} p(\boldsymbol{Y} \mid \boldsymbol{\theta})p(\boldsymbol{\theta}) d\boldsymbol{\theta}} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(\Theta\)</span> 表示我们正在积分 <span class="math notranslate nohighlight">\(\theta\)</span> 的所有可能值。</p>
<p>像这样计算积分非常困难（ 参见 <a class="reference internal" href="chp_11.html#marginal-likelihood"><span class="std std-ref">11.7 边缘似然</span></a> 和一个有趣的 <code class="docutils literal notranslate"><span class="pre">XKCD</span> <span class="pre">漫画</span></code> <a class="footnote-reference brackets" href="#id49" id="id11">6</a> ）。尤其是对于大多数问题而言，根本无法给出边缘似然的封闭解，计算积分就更困难了。幸运的是，有一些数值方法可以帮助我们应对这一挑战。</p>
<p>在实践中，很多问题的解决并不需要计算边缘似然，此时将贝叶斯定理表示为比率形式比较常见：</p>
<div class="math notranslate nohighlight" id="equation-eq-proportional-bayes">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-proportional-bayes" title="Permalink to this equation">¶</a></span>\[\underbrace{p(\boldsymbol{\theta} \mid \boldsymbol{Y})}_{\text{posterior}} \propto \overbrace{p(\boldsymbol{Y} \mid \boldsymbol{\theta})}^{\text{likelihood}}\; \overbrace{p(\boldsymbol{\theta})}^{\text{prior}}\]</div>
<div class="admonition- admonition">
<p class="admonition-title">关于符号的说明</p>
<p>在本书中，我们使用相同的符号 <span class="math notranslate nohighlight">\(p(\cdot)\)</span> 来表示不同的量，例如：似然函数和先验概率分布。这是对符号的轻微滥用（ 似然函数并非一定是某种概率分布 ），但这样做很有用。这种符号表示为贝叶斯公式中的所有量提供了相同的认识论地位。此外，它还反映出：即便似然不是严格意义上的概率密度函数，我们也不在乎，因为我们只考虑先验背景下的似然，反之亦然。换句话说，为了计算后验分布，我们将这两个量视为模型的同等必要元素。</p>
</div>
<p>贝叶斯统计的特点之一是：<strong>后验（总是）是一个概率分布</strong>。这使我们能够对参数做出概率性表述，例如参数 <span class="math notranslate nohighlight">\(\boldsymbol{\tau}\)</span> 为正的概率是 <span class="math notranslate nohighlight">\(0.35\)</span> 。或者 <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span> 的最可能值是 <span class="math notranslate nohighlight">\(12\)</span>，并且有 <span class="math notranslate nohighlight">\(50\%\)</span> 的机会介于 <span class="math notranslate nohighlight">\(10\)</span> 和 <span class="math notranslate nohighlight">\(15\)</span> 之间。</p>
<p>此外，可以将后验分布视为将模型与数据相结合的逻辑性结果，因此由其得出的概率陈述保证了在数学上的一致性。我们只需记住，所有这些好的数学性质只在柏拉图式的思想世界中有效。当我们从数学纯粹性转向现实世界中应用数学的混杂性时，必须始终牢记：<strong>我们的结果不仅取决于数据，还取决于模型</strong>。因此，不良数据和/或不良模型均可能导致无意义的陈述，即使其在数学上是一致的。我们必须始终对数据、模型和结果保持健康的怀疑精神。</p>
<p>为了使这一点更明确，我们可以更准确地表示贝叶斯定理如下：</p>
<div class="math notranslate nohighlight">
\[p(\boldsymbol{\theta} \mid  \boldsymbol{Y}, M) \propto  p(\boldsymbol{Y} \mid \boldsymbol{\theta}, M) \; p(\boldsymbol{\theta}, M)\]</div>
<p>上式强调了我们的推断总是依赖于模型 <span class="math notranslate nohighlight">\(M\)</span> 所做的假设。</p>
<p>一旦有了后验分布，我们就可以用它来推导其他感兴趣的量，而这通常以计算期望的方式实现，例如：</p>
<div class="math notranslate nohighlight" id="equation-eq-posterior-expectation">
<span class="eqno">(4)<a class="headerlink" href="#equation-eq-posterior-expectation" title="Permalink to this equation">¶</a></span>\[J = \int f(\boldsymbol{\theta}) \; 
p(\boldsymbol{\theta} \mid \boldsymbol{Y}) \; 
d\boldsymbol{\theta} \]</div>
<p>如果 <span class="math notranslate nohighlight">\(f\)</span> 为恒等函数，则 <span class="math notranslate nohighlight">\(J\)</span> 是 <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> 的均值 <a class="footnote-reference brackets" href="#id50" id="id12">7</a>：</p>
<div class="math notranslate nohighlight">
\[\bar{\boldsymbol{\theta}} = \int_{\boldsymbol{\Theta}} \boldsymbol{\theta}  p(\boldsymbol{\theta} \mid \boldsymbol{Y})  d\boldsymbol{\theta}\]</div>
<p>后验分布是贝叶斯统计的核心对象，但不是唯一重要的。除了对参数值进行推断外，我们可能还想对数据做出推断。这可以通过计算<strong>先验预测分布</strong>来完成：</p>
<div class="math notranslate nohighlight" id="equation-eq-prior-pred-dist">
<span class="eqno">(5)<a class="headerlink" href="#equation-eq-prior-pred-dist" title="Permalink to this equation">¶</a></span>\[p(\boldsymbol{Y}^\ast) =  \int_{\boldsymbol{\Theta}} p(\boldsymbol{Y^\ast} \mid \boldsymbol{\theta}) \; p(\boldsymbol{\theta}) \; d\boldsymbol{\theta} \]</div>
<p>这是根据模型（先验和似然）做出的预期数据分布。给定模型，这是在实际看到任何观测数据 <span class="math notranslate nohighlight">\(\boldsymbol{Y}^\ast\)</span> 之前所预期的数据。</p>
<p>请注意，公式 <a class="reference internal" href="#equation-eq-marginal-likelihood">(2)</a>（边缘似然）和公式 <a class="reference internal" href="#equation-eq-prior-pred-dist">(5)</a>（先验预测分布）看起来有些相似，但其含义是截然不同的。在边缘似然公式中，我们以观测数据 <span class="math notranslate nohighlight">\(Y\)</span> 为条件，而在先验预测分布公式中，我们并不以观测数据为条件。最终，边缘似然表现为一个数字，而先验预测分布则是一个概率分布。</p>
<p>我们可以使用先前预测分布的样本作为评估和校准模型的一种方式。例如，我们可能会问 “人类身高的模型能否将人类身高预测为 <span class="math notranslate nohighlight">\(-1.5\)</span> 米？”之类的问题。而这种问题在测量身高之前，我们就能认识其荒谬性。在本书后面章节中，我们将看到许多使用先验预测分布进行模型评估的示例，以及先验预测分布如何为后续建模选择提供有效或无效信息。</p>
<div class="admonition- admonition">
<p class="admonition-title">贝叶斯模型作为生成式模型</p>
<p>采用概率视角进行建模导致了一个口头禅：<em>模型生成数据</em> <span id="id13">[<a class="reference internal" href="references.html#id28">4</a>]</span>。我们认为此概念至关重要。一旦你将它内化，所有统计模型都会变得更加清晰，甚至是非贝叶斯模型。</p>
<p>此口头禅可以帮助我们创建新模型；如果数据是由模型生成的，那么我们可以 <em>仅</em> 通过考虑如何生成数据，来为数据创建适合的模型！此外，此口头禅并不是一个抽象概念，我们可以用先验预测分布作为其具体表示。如果重新审视贝叶斯建模的三个步骤，我们可以将它们重新调整为：编写先验预测分布 –&gt; 添加数据以对其进行约束 —&gt; 检查结果是否有意义。</p>
<p>当然，必要时同样需要进行迭代。</p>
</div>
<p>另一个需要计算的量是<strong>后验预测分布</strong>：</p>
<div class="math notranslate nohighlight" id="equation-eq-post-pred-dist">
<span class="eqno">(6)<a class="headerlink" href="#equation-eq-post-pred-dist" title="Permalink to this equation">¶</a></span>\[p(\tilde{\boldsymbol{Y}} \mid \boldsymbol{Y}) = \int_{\boldsymbol{\Theta}} p(\tilde{\boldsymbol{Y}} \mid \boldsymbol{\theta}) \, p(\boldsymbol{\theta} \mid \boldsymbol{Y}) \, d\boldsymbol{\theta} \]</div>
<p>这是根据后验 <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta} \mid \boldsymbol{Y})\)</span> 预期的未来数据 <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{Y}}\)</span> 的分布；而后验又是模型（先验和似然）和观测数据的结果。用更常见的术语来说，这是模型在看到数据集 <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span> 后期望看到的未来数据，即这些是模型的预测。</p>
<p>从公式 <a class="reference internal" href="#equation-eq-post-pred-dist">(6)</a> 可以看到，预测是通过对参数的后验分布进行积分（或边缘化）来计算的。因此，以这种预测包含了估计的不确定性。</p>
<div class="admonition- admonition">
<p class="admonition-title">频率主义者眼中的贝叶斯后验</p>
<p>因为后验仅来自于模型和观测数据，所以我们并不是基于未观测到的事情做出陈述，而是基于内蕴数据生成过程得到的潜在观测做出陈述。</p>
<p>对未观测的事物做出推断通常是频率主义者的方法。但如果使用后验预测样本来检查模型，我们其实（部分）接受了频率主义者关于“未观测但潜在可观测的数据”的思想。</p>
<p>我们不仅对该想法满意，而且将在本书中看到此过程的多处示例。我们认为这是一个很棒的主意！</p>
</div>
</div>
</div>
<div class="section" id="sampling-methods-intro">
<span id="id14"></span><h2>1.2 一个自制的采样器<a class="headerlink" href="#sampling-methods-intro" title="Permalink to this headline">¶</a></h2>
<p>公式 <a class="reference internal" href="#equation-eq-marginal-likelihood">(2)</a> 中的积分很多情况下没有封闭形式解，因此现代贝叶斯推断大多使用被称为 <strong>通用推断引擎（ Universal Inference Engines ）</strong> 的数值方法来实现（ 参见 <a class="reference internal" href="chp_11.html#inference-methods"><span class="std std-ref">11.9 推断方法</span></a> ) 。有许多经过良好测试的 Python 库能够提供此类数值方法，因此一般来说，贝叶斯实践者不太可能需要编写自己的通用推断引擎。</p>
<p>目前，编写自己的引擎通常只有两个理由：一是设计一个能够改进旧引擎的新引擎；二是正在学习当前引擎的工作原理。本章出于学习目的，我们将编写一个代码，但对于本书其余部分，主要使用 Python 库中的可用引擎。</p>
<p>有许多算法可以用于通用推断引擎，其中使用最广泛、功能最强的算法是<code class="docutils literal notranslate"><span class="pre">马尔可夫链蒙特卡洛方法（</span> <span class="pre">MCMC</span> <span class="pre">）</span></code> 。在较高层次上，所有 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code> 都使用样本来近似后验分布，而这些样本大多通过接受或拒绝来自某个提议分布的样本来生成。通过遵循某些规则和假设，我们有理论上的保证，能够获得非常近似后验分布的样本 <a class="footnote-reference brackets" href="#id51" id="id15">8</a> 。因此，MCMC 方法也称为<strong>采样器 （ Sampler ）</strong>。所有这些方法都需要具有估计给定参数值时的先验和似然的能力。也就是说，即使不知道完整的后验形态，我们也能够逐点获取其概率密度。</p>
<p>此类算法之一是 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> <span id="id16">[<a class="reference internal" href="references.html#id106">7</a>, <a class="reference internal" href="references.html#id107">8</a>, <a class="reference internal" href="references.html#id108">9</a>]</span>。这并不是一个非常现代或有效的算法，但很容易被理解，因此为理解更复杂、更强大的其他方法奠定了基础 <a class="footnote-reference brackets" href="#id52" id="id17">9</a> 。</p>
<p><code class="docutils literal notranslate"><span class="pre">Metropolis-Hasting</span></code> 算法定义如下：</p>
<ol>
<li><p>在 <span class="math notranslate nohighlight">\(x_i\)</span> 处初始化参数 <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> 的值</p></li>
<li><p>使用提议分布 <span class="math notranslate nohighlight">\(q(x_{i + 1} \mid x_i)\)</span> 从旧值 <span class="math notranslate nohighlight">\(x_i\)</span> 生成新值 <span class="math notranslate nohighlight">\(x_{i + 1}\)</span> <a class="footnote-reference brackets" href="#id53" id="id18">10</a>。</p></li>
<li><p>计算新值被接受的概率：</p>
<div class="math notranslate nohighlight" id="equation-acceptance-prob">
<span class="eqno">(7)<a class="headerlink" href="#equation-acceptance-prob" title="Permalink to this equation">¶</a></span>\[p_a (x_{i + 1} \mid x_i) = \min \left (1, \frac{p(x_{i + 1}) \; q(x_i \mid x_{i + 1})} {p(x_i) \; q (x_{i + 1} \mid x_i)} \right)\]</div>
</li>
<li><p>如果 <span class="math notranslate nohighlight">\(p_a &gt; R\)</span> 其中 <span class="math notranslate nohighlight">\(R \sim \mathcal{U}(0, 1)\)</span>，则保留新值，否则保留旧值。</p></li>
<li><p>迭代 <span class="math notranslate nohighlight">\(2\)</span> 到 <span class="math notranslate nohighlight">\(4\)</span> 直到生成 <em>足够多</em> 的样本点。</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">Metropolis</span> <span class="pre">算法</span></code> 非常通用，可以在非贝叶斯应用中使用，但对于本书的内容，<span class="math notranslate nohighlight">\(p(x_i)\)</span> 是在参数值 <span class="math notranslate nohighlight">\(x_i\)</span> 处估计的后验密度。请注意，如果 <span class="math notranslate nohighlight">\(q\)</span> 是对称分布，则 <span class="math notranslate nohighlight">\(q(x_i \mid x_{i + 1})\)</span> 和 <span class="math notranslate nohighlight">\(q(x_{i + 1} \mid x_i)\)</span> 将被消掉（ 从概念上讲，这意味着从 <span class="math notranslate nohighlight">\(x_{i+1}\)</span> 转移到 <span class="math notranslate nohighlight">\(x_i\)</span> 或从 <span class="math notranslate nohighlight">\(x_{i}\)</span> 到 <span class="math notranslate nohighlight">\(x_{i+1}\)</span> 具有相同的可能性 )，只留下在两个点处估计的后验比率。从公式 <a class="reference internal" href="#equation-acceptance-prob">(7)</a> 可以看到该算法将始终接受从低概率区到较高概率区的转移，并且将按照概率接受从高概率区到低概率区的移动。</p>
<p>另一个重要说明是: <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code>不是一种优化方法！我们并不关心最大概率的点在哪儿，而是想探索 <span class="math notranslate nohighlight">\(p\)</span> 分布（ 贝叶斯统计中主要指后验分布 ）。如果你稍加注意，就会发现此方法在达到最大概率区后并不会停止，而是在后续步骤中继续转移到较低概率区。</p>
<p>为了使事情更具象，让我们尝试求解 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code>。这可能是贝叶斯统计中最常见的示例，它用于对二值、互斥的结果进行建模，例如 <code class="docutils literal notranslate"><span class="pre">0</span></code> 或 <code class="docutils literal notranslate"><span class="pre">1</span></code>、<code class="docutils literal notranslate"><span class="pre">正</span></code>或<code class="docutils literal notranslate"><span class="pre">负</span></code>、<code class="docutils literal notranslate"><span class="pre">正面</span></code>或<code class="docutils literal notranslate"><span class="pre">反面</span></code>、<code class="docutils literal notranslate"><span class="pre">垃圾邮件</span></code>或<code class="docutils literal notranslate"><span class="pre">正常邮件</span></code>、<code class="docutils literal notranslate"><span class="pre">热狗</span></code>或<code class="docutils literal notranslate"><span class="pre">非热狗</span></code>、<code class="docutils literal notranslate"><span class="pre">健康</span></code>或<code class="docutils literal notranslate"><span class="pre">不健康</span></code>等。 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code> 经常被用作介绍贝叶斯统计基础知识的第一个示例，因为它是一个简单的模型，可以轻松求解和计算。在统计符号中 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code> 记为：</p>
<div class="math notranslate nohighlight" id="equation-eq-beta-binomial">
<span class="eqno">(8)<a class="headerlink" href="#equation-eq-beta-binomial" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
\theta \sim &amp;\; \text{Beta}(\alpha, \beta) \\
Y \sim &amp;\; \text{Bin}(n=1, p=\theta) 
\end{split}\end{split}\]</div>
<p>在公式 <a class="reference internal" href="#equation-eq-beta-binomial">(8)</a> 中，未知参数为 <span class="math notranslate nohighlight">\(\theta\)</span> ，其先验分布为贝塔分布 <span class="math notranslate nohighlight">\(\text{Beta}(\alpha, \beta)\)</span> ；我们假设数据的似然函数为二项分布 <span class="math notranslate nohighlight">\(\text{Bin}(n=1, p=\theta)\)</span> 。在此模型中，成功的数量 <span class="math notranslate nohighlight">\(\theta\)</span> 可以代表抛硬币时的正面比例、病亡率等量。该模型实际上存在解析形式解（ 参见 <a class="reference internal" href="#conjugate-priors"><span class="std std-ref">1.4.1 共轭先验</span></a> 了解详细信息 ），但为了讲解，我们假设现在并不知道如何计算后验。因此我们将在 Python 代码中实现 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code>来以获得近似解。我们将在 SciPy 的统计函数支持下实现：</p>
<div class="literal-block-wrapper docutils container" id="metropolis-hastings-sampler">
<div class="code-block-caption"><span class="caption-number">Listing 1 </span><span class="caption-text">Metropolis_Hhastings_采样器</span><a class="headerlink" href="#metropolis-hastings-sampler" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">post</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">β</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">θ</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">prior</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html#scipy.stats.beta" title="scipy.stats.beta"><span class="n">stats</span><span class="o">.</span><span class="n">beta</span></a><span class="p">(</span><span class="n">α</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">θ</span><span class="p">)</span>
        <span class="n">like</span>  <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html#scipy.stats.bernoulli" title="scipy.stats.bernoulli"><span class="n">stats</span><span class="o">.</span><span class="n">bernoulli</span></a><span class="p">(</span><span class="n">θ</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">like</span> <span class="o">*</span> <span class="n">prior</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="o">-</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/constants.html#numpy.inf" title="numpy.inf"><span class="n">np</span><span class="o">.</span><span class="n">inf</span></a>
    <span class="k">return</span> <span class="n">prob</span>
</pre></div>
</div>
</div>
<p>实现后验推断还需要数据，为此可以随机生成一些合成数据（也称伪数据）。</p>
<div class="literal-block-wrapper docutils container" id="metropolis-hastings-sampler-rvs">
<div class="code-block-caption"><span class="caption-number">Listing 2 </span><span class="caption-text">metropolis_hastings_sampler_rvs</span><a class="headerlink" href="#metropolis-hastings-sampler-rvs" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bernoulli.html#scipy.stats.bernoulli" title="scipy.stats.bernoulli"><span class="n">stats</span><span class="o">.</span><span class="n">bernoulli</span></a><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>运行 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code>的实现代码：</p>
<div class="literal-block-wrapper docutils container" id="metropolis-hastings">
<div class="code-block-caption"><span class="caption-number">Listing 3 </span><span class="caption-text">metropolis_hastings</span><a class="headerlink" href="#metropolis-hastings" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">n_iters</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="linenos"> 2</span><span class="n">can_sd</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="linenos"> 3</span><span class="n">α</span> <span class="o">=</span> <span class="n">β</span> <span class="o">=</span>  <span class="mi">1</span>
<span class="linenos"> 4</span><span class="n">θ</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="linenos"> 5</span><span class="n">trace</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"θ"</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_iters</span><span class="p">)}</span>
<span class="linenos"> 6</span><span class="n">p2</span> <span class="o">=</span> <span class="n">post</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">α</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iters</span><span class="p">):</span>
<span class="linenos"> 9</span>    <span class="n">θ_can</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">can_sd</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">10</span>    <span class="n">p1</span> <span class="o">=</span> <span class="n">post</span><span class="p">(</span><span class="n">θ_can</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">α</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span>
<span class="linenos">11</span>    <span class="n">pa</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">/</span> <span class="n">p2</span>
<span class="linenos">12</span>
<span class="linenos">13</span>    <span class="k">if</span> <span class="n">pa</span> <span class="o">&gt;</span> <span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
<span class="linenos">14</span>        <span class="n">θ</span> <span class="o">=</span> <span class="n">θ_can</span>
<span class="linenos">15</span>        <span class="n">p2</span> <span class="o">=</span> <span class="n">p1</span>
<span class="linenos">16</span>
<span class="linenos">17</span>    <span class="n">trace</span><span class="p">[</span><span class="s2">"θ"</span><span class="p">][</span><span class="nb">iter</span><span class="p">]</span> <span class="o">=</span> <span class="n">θ</span>
</pre></div>
</div>
</div>
<p>在代码 <a class="reference internal" href="#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> 中，第 <span class="math notranslate nohighlight">\(9\)</span> 行从标准差为 <code class="docutils literal notranslate"><span class="pre">can_sd</span></code> 的正态分布中采样来生成提议分布。第 <span class="math notranslate nohighlight">\(10\)</span> 行在新生成的值 <code class="docutils literal notranslate"><span class="pre">θ_can</span></code> 处估计后验，第 <span class="math notranslate nohighlight">\(11\)</span> 行计算接受概率。第 <span class="math notranslate nohighlight">\(17\)</span> 行在 <code class="docutils literal notranslate"><span class="pre">trace</span></code> 数组中保存 <code class="docutils literal notranslate"><span class="pre">θ</span></code> 的值。</p>
<p>此值是新值还是重复上一个值，取决于第 <span class="math notranslate nohighlight">\(13\)</span> 行的比较结果。</p>
<div class="admonition-mcmc admonition">
<p class="admonition-title">模棱两可的 MCMC 术语 </p>
<p>当使用马尔可夫链蒙特卡洛方法进行贝叶斯推断时，我们通常将其称为<strong>MCMC 采样器</strong>。在每次迭代中，我们从采样器中抽取一个随机样本，因此很自然地将 MCMC 的结果称为 <em>样本（ Samples ）</em> 或 <em>抽取（Draws）</em>。有些人将<em>样本</em>视为由一组<em>抽取</em>组成，而另外有一些人倾向于两者可以互换。</p>
<p>由于 MCMC 是按顺序抽取样本的，因此也会说*我们得到了一个抽取结果的 <em>链（ Chain ）</em>，或者简称为 MCMC 链。出于计算和诊断的原因，通常需要抽取许多链（ 我们在<a class="reference internal" href="chp_02.html#chap1bis"><span class="std std-ref">第 2 章</span></a> 中讨论了如何做到这一点 ）。所有输出链，无论是单数还是复数，通常都被称为轨迹或简单地称为后验。不幸的是，口语并不精确，如果需要精确，最好的方法是查看代码以准确了解正在发生的事情。</p>
</div>
<p>请注意，代码 <a class="reference internal" href="#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> 中的实现并非旨在提高效率，实际上生产级代码中会出现许多调整，例如计算对数规模的概率以避免溢出问题等（ 参见 <a class="reference internal" href="chp_10.html#log-probabilities"><span class="std std-ref">10.4.1 对数概率</span></a> 部分 ），或预先计算提议分布值。这些都是需要改变数学纯粹性以适应计算机实现的地方，也解释了为什么最好让专家来构建这些引擎。</p>
<p>同样， <code class="docutils literal notranslate"><span class="pre">can_sd</span></code> 的值是 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code> 的参数，而不是贝叶斯模型的参数。理论上该参数不应该影响算法的正确行为，但在实践中它又非常重要，因为方法效率肯定会受到其值的影响（ 参阅 <a class="reference internal" href="chp_11.html#inference-methods"><span class="std std-ref">11.9 推断方法</span></a>  ）。</p>
<p>回到示例，现在有了 MCMC 样本，我们想了解其形态。检查贝叶斯推断结果的一种常用方法是将每次迭代得到的采样值通过直方图或其他可视化工具绘制出来，以表示分布。例如，可以使用代码 <a class="reference internal" href="#diy-trace-plot"><span class="std std-ref">diy_trace_plot</span></a> 中的代码来绘制 <a class="reference internal" href="#fig-traceplot"><span class="std std-numref">Fig. 2</span></a> <a class="footnote-reference brackets" href="#id54" id="id19">11</a>：</p>
<div class="literal-block-wrapper docutils container" id="diy-trace-plot">
<div class="code-block-caption"><span class="caption-number">Listing 4 </span><span class="caption-text">diy_trace_plot</span><a class="headerlink" href="#diy-trace-plot" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="s2">"θ"</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">"0.5"</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s2">"horizontal"</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-traceplot">
<a class="reference internal image-reference" href="../_images/traceplot.png"><img alt="../_images/traceplot.png" src="../_images/traceplot.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">左图中，每次迭代都会产生参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的采样值。右图为 <span class="math notranslate nohighlight">\(\theta\)</span> 采样值的直方图。该直方图经过了旋转，以便更容易看出两个图之间的密切关系。左图显示了采样值的顺序序列，该序列其实就是所谓的马尔可夫链，而右图则显示了采样值的分布情况。</span><a class="headerlink" href="#fig-traceplot" title="Permalink to this image">¶</a></p>
</div>
<p>通常，计算一些数字汇总信息也很有用。我们将使用名为 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 的 Python 软件包 <span id="id20">[<a class="reference internal" href="references.html#id11">3</a>]</span> 来计算这些统计信息：</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><a class="sphinx-codeautolink-a" href="https://arviz-devs.github.io/arviz/api/generated/arviz.summary.html#arviz.summary" title="arviz.summary"><span class="n">az</span><span class="o">.</span><span class="n">summary</span></a><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">"stats"</span><span class="p">,</span> <span class="n">round_to</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<table class="table" id="tab-posterior-summary">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">后验分布的汇总信息</span><a class="headerlink" href="#tab-posterior-summary" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%"/>
<col style="width: 20%"/>
<col style="width: 20%"/>
<col style="width: 20%"/>
<col style="width: 20%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\theta\)</span></p></td>
<td><p>0.69</p></td>
<td><p>0.01</p></td>
<td><p>0.52</p></td>
<td><p>0.87</p></td>
</tr>
</tbody>
</table>
<p>ArviZ 的函数 <code class="docutils literal notranslate"><span class="pre">summary</span></code> 计算参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的均值、标准差和 <span class="math notranslate nohighlight">\(94\%\)</span> 最高密度区间 (HDI)。 最高密度区间（ HDI ）是包含给定概率密度（此处为 <span class="math notranslate nohighlight">\(94\%\)</span> ）的最短区间 <a class="footnote-reference brackets" href="#id55" id="id21">12</a> 。 <a class="reference internal" href="#fig-plot-posterior"><span class="std std-numref">Fig. 3</span></a> 为采用 <code class="docutils literal notranslate"><span class="pre">az.plot_posterior(trace)</span></code> 生成，其与 <a class="reference internal" href="#tab-posterior-summary"><span class="std std-numref">Table 1</span></a> 中的汇总信息非常相似。我们可以在代表整个后验分布的曲线顶部看到均值和最高密度区间。该曲线是使用 <strong>核密度估计（ Kernel Density Estimator, KDE ）</strong> 计算的，类似于直方图的平滑版本。</p>
<p>ArviZ 在许多绘图函数中都会使用 KDE，甚至在内部进行一些计算。</p>
<div class="figure align-default" id="fig-plot-posterior">
<a class="reference internal image-reference" href="../_images/plot_posterior.png"><img alt="../_images/plot_posterior.png" src="../_images/plot_posterior.png" style="width: 4in;"/></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">用后验图对代码 <a class="reference internal" href="#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> 生成的样本进行可视化。后验分布使用 KDE 表示，均值和 <span class="math notranslate nohighlight">\(94\%\)</span> 最高密度区间均在图中有所展示。</span><a class="headerlink" href="#fig-plot-posterior" title="Permalink to this image">¶</a></p>
</div>
<p>最高密度区间（ HDI ）是贝叶斯统计中的常见选择，像 <span class="math notranslate nohighlight">\(50\%\)</span> 或 <span class="math notranslate nohighlight">\(95\%\)</span> 这样的边界值也很常见。但是 ArviZ 的默认值为 <span class="math notranslate nohighlight">\(94\%\)</span>（ 或 <span class="math notranslate nohighlight">\(0.94\)</span> ），如 <a class="reference internal" href="#tab-posterior-summary"><span class="std std-numref">Table 1</span></a> 和 <a class="reference internal" href="#fig-plot-posterior"><span class="std std-numref">Fig. 3</span></a> 中所示。这种选择的原因是 <span class="math notranslate nohighlight">\(94\)</span> 接近广泛使用的 <span class="math notranslate nohighlight">\(95\)</span>，而同时这种差别可以友好地提醒观众，边界值的选择并没有什么特别之处 <span id="id22">[<a class="reference internal" href="references.html#id33">10</a>]</span> 。理想情况下，你应该选择一个适合需要的值 <span id="id23">[<a class="reference internal" href="references.html#id99">11</a>]</span>，或者至少承认你使用的是默认值。</p>
</div>
<div class="section" id="automating-inference">
<span id="id24"></span><h2>1.3 人工建模与自动推断<a class="headerlink" href="#automating-inference" title="Permalink to this headline">¶</a></h2>
<p>我们可以利用 <strong>概率编程语言 Probabilistic Programming Languages</strong> (PPL) 的帮助，而不是编写自己的采样器从头定义自己的模型。概率编程语言允许用户使用代码表达贝叶斯模型，然后借助通用推断引擎以相当自动化的方式执行贝叶斯推断。简而言之，PPL 帮助贝叶斯实践者更多地关注模型构建本身，而不是数学和计算细节。</p>
<p>在过去的几十年中，此类工具的可用性大大提升了贝叶斯方法的普及度和实用性。不幸的是，这些通用推断引擎方法并不是真正通用的，因为它们无法有效地解决所有贝叶斯模型。现代贝叶斯实践者的部分工作是能够理解并且9999解决这些限制。</p>
<p>在本书中，我们将使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> <span id="id25">[<a class="reference internal" href="references.html#id124">1</a>]</span> 和 <code class="docutils literal notranslate"><span class="pre">TensorFlow</span> <span class="pre">Probability</span></code> <span id="id26">[<a class="reference internal" href="references.html#id141">2</a>]</span>。让我们使用 PyMC3 为公式 <a class="reference internal" href="#equation-eq-beta-binomial">(8)</a> 编写模型：</p>
<div class="literal-block-wrapper docutils container" id="beta-binom">
<div class="code-block-caption"><span class="caption-number">Listing 5 </span><span class="caption-text">beta_binom</span><a class="headerlink" href="#beta-binom" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Declare a model in PyMC3 </span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>     
   <span class="c1"># Specify the prior distribution of unknown parameter</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s2">"θ"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
   <span class="c1"># Specify the likelihood distribution and condition on the observed data     </span>
    <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s2">"y_obs"</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span> 

<span class="c1"># Sample from the posterior distribution     </span>
<span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<p>你可以自己检查一下这段代码的结果是否与之前自制采样器的结果一致，但在 PPL 支持下，工作量要少得多。如果你不熟悉 PyMC3 语法，现阶段只需关注代码注释中表达的每一行的意图。</p>
<p>由于我们已经在 PyMC3 语法中定义了模型，因此可以利用 <code class="docutils literal notranslate"><span class="pre">pm.model_to_graphviz(model)</span></code> 在代码 <a class="reference internal" href="#beta-binom"><span class="std std-ref">beta_binom</span></a> 中生成模型的概率图表示（参见 <a class="reference internal" href="#fig-betabinommodelgraphviz"><span class="std std-numref">Fig. 4</span></a>）。</p>
<div class="figure align-default" id="fig-betabinommodelgraphviz">
<a class="reference internal image-reference" href="../_images/BetaBinomModelGraphViz.png"><img alt="../_images/BetaBinomModelGraphViz.png" src="../_images/BetaBinomModelGraphViz.png" style="width: 2in;"/></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">公式 <a class="reference internal" href="#equation-eq-beta-binomial">(8)</a> 和代码 <a class="reference internal" href="#beta-binom"><span class="std std-ref">beta_binom</span></a> 中定义的贝叶斯模型的概率图表示。椭圆代表先验和似然，而 <span class="math notranslate nohighlight">\(20\)</span> 表示观测次数。</span><a class="headerlink" href="#fig-betabinommodelgraphviz" title="Permalink to this image">¶</a></p>
</div>
<p>概率编程语言不仅可以估计随机变量的对数概率以获得后验分布，还可以模拟各种预测分布。例如，代码 <a class="reference internal" href="#predictive-distributions"><span class="std std-ref">predictive_distributions</span></a> 展示了如何使用 PyMC3 获得先验预测分布的 <span class="math notranslate nohighlight">\(1000\)</span> 个样本，以及后验预测分布的 <span class="math notranslate nohighlight">\(1000\)</span> 个样本。请注意，第一个函数有一个 <code class="docutils literal notranslate"><span class="pre">model</span></code> 参数，而第二个函数必须同时传递 <code class="docutils literal notranslate"><span class="pre">model</span></code> 和 <code class="docutils literal notranslate"><span class="pre">trace</span></code> 参数，这反映了先验预测分布仅需要模型，而后验预测分布不仅需要模型还需要后验分布。从先验预测分布和后验预测分布生成的样本分别在 <a class="reference internal" href="#fig-quartet"><span class="std std-numref">Fig. 5</span></a> 的顶部和底部子图中表示。</p>
<div class="literal-block-wrapper docutils container" id="predictive-distributions">
<div class="code-block-caption"><span class="caption-number">Listing 6 </span><span class="caption-text">predictive_distributions</span><a class="headerlink" href="#predictive-distributions" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_dists</span> <span class="o">=</span> <span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">model</span><span class="p">)[</span><span class="s2">"y_obs"</span><span class="p">],</span>
              <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">model</span><span class="p">)[</span><span class="s2">"y_obs"</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>公式 <a class="reference internal" href="#equation-eq-posterior-dist">(1)</a> 、 <a class="reference internal" href="#equation-eq-prior-pred-dist">(5)</a> 和 <a class="reference internal" href="#equation-eq-post-pred-dist">(6)</a> 清楚地将后验分布、先验预测分布和后验预测分布定义为三个不同的数学对象。后面的两个是数据分布，而第一个是参数的分布。<a class="reference internal" href="#fig-quartet"><span class="std std-numref">Fig. 5</span></a> 帮助我们可视化了这种差异，为了完整，该图还包括了先验分布。</p>
<div class="admonition- admonition">
<p class="admonition-title">用多种方式表示模型</p>
<p>有许多方法可以表示统计模型的架构。以下没有特定的顺序：</p>
<ul class="simple">
<li><p>口语和书面语言</p></li>
<li><p>概念图：<a class="reference internal" href="#fig-betabinommodelgraphviz"><span class="std std-numref">Fig. 4</span></a>。</p></li>
<li><p>数学符号：公式 <a class="reference internal" href="#equation-eq-beta-binomial">(8)</a></p></li>
<li><p>计算机代码：代码 <a class="reference internal" href="#beta-binom"><span class="std std-ref">beta_binom</span></a></p></li>
</ul>
<p>对于现代贝叶斯实践者来说，了解所有这些媒介很有用。它们是你在会谈、科学论文、与同事讨论时的手绘草图、互联网上的代码示例等中经常看到的格式。熟练地使用这些媒介，你将能够更好地理解以某种方式呈现的概念，然后将其应用到另一种方式中。例如，阅读一篇论文然后实现一个模型，或者在演讲中听到一种技术，然后能够为其写一篇博客。对个人而言，熟练程度会加快你的学习速度并提高与他人交流的能力。</p>
</div>
<div class="figure align-default" id="fig-quartet">
<a class="reference internal image-reference" href="../_images/Bayesian_quartet_distributions.png"><img alt="../_images/Bayesian_quartet_distributions.png" src="../_images/Bayesian_quartet_distributions.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">自上至下，我们展示了：（1）参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的先验分布样本； (2) 成功总数的先验预测分布样本； (3) 参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的后验分布样本； (4) 成功总数的后验预测分布样本。在第一个和第三个图、第二个和第四个图之间分别共享 <span class="math notranslate nohighlight">\(x\)</span> 轴和 <span class="math notranslate nohighlight">\(y\)</span> 轴的坐标尺度。</span><a class="headerlink" href="#fig-quartet" title="Permalink to this image">¶</a></p>
</div>
<p>正如我们已经提到的，后验预测分布考虑了估计结果的不确定性。</p>
<p><a class="reference internal" href="#fig-predictions-distributions"><span class="std std-numref">Fig. 6</span></a> 表明根据均值得到的预测，比根据后验预测分布得到的预测更窄。该结果不仅对均值有效，对于其他任何点估计，会得到类似的图。</p>
<div class="figure align-default" id="fig-predictions-distributions">
<a class="reference internal image-reference" href="../_images/predictions_distributions.png"><img alt="../_images/predictions_distributions.png" src="../_images/predictions_distributions.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code>的预测结果，使用后验均值所做的预测表示为灰色直方图，使用完整后验所做的预测（ 即后验预测分布 ）表示为蓝色直方图。</span><a class="headerlink" href="#fig-predictions-distributions" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="make-prior-count">
<span id="id27"></span><h2>1.4 量化先验信息的几种选择<a class="headerlink" href="#make-prior-count" title="Permalink to this headline">¶</a></h2>
<p>在贝叶斯统计中，不得不选择先验分布这件事，既是一种负担又是一种祝福，而我们认为这是必要的。如果你没有选择先验，那么最大可能是别人正在为你做这件事。当然，让别人替你做决定并不总是一件坏事。如果在正确的场景中应用并且意识到其局限性，许多非贝叶斯方法可能非常有用和有效。然而，我们坚信：<strong>了解模型假设并灵活调整它们是有优势的，先验只是假设的一种形式</strong>。</p>
<p>我们也明白，对于许多实践者来说，先验选择可能是怀疑、焦虑甚至沮丧的根源，尤其是对于新手，但不仅限于新人。寻找给定问题的最佳先验，是一个常见且完全有效的问题。但是除了没有最佳先验此结论之外，很难给出一个令人满意的答案。好在我们有一些默认值，可以作为迭代式建模工作流的起点。</p>
<p>在本节中，我们将讨论一些选择先验的一般性方法。此讨论或多或少遵循一个信息性的阶梯，从不包含任何信息的“空白”先验，到信息丰富的、信息尽可能多的先验。</p>
<p>本章关于先验的讨论更多是在理论方面的。在后续章节中，我们将讨论如何在更实际的环境中选择先验。</p>
<div class="section" id="conjugate-priors">
<span id="id28"></span><h3>1.4.1 共轭先验<a class="headerlink" href="#conjugate-priors" title="Permalink to this headline">¶</a></h3>
<p>如果后验与先验属于同一分布族，则先验与似然共轭，或称其为似然的共轭先验。例如，如果似然是 <code class="docutils literal notranslate"><span class="pre">Poisson</span></code> 的并且先验为 <code class="docutils literal notranslate"><span class="pre">Gamma</span></code> 分布，那么后验也会是 <code class="docutils literal notranslate"><span class="pre">Gamma</span></code> 分布 <a class="footnote-reference brackets" href="#id56" id="id29">13</a>。</p>
<p>从纯数学角度来看，<strong>共轭先验</strong>是最有利的选择，因为共轭先验和似然能够得到后验的封闭形式表达式，这允许我们用“纸笔”就可以分析和计算后验分布 <a class="footnote-reference brackets" href="#id57" id="id30">14</a>。然而，从现代计算角度来看，共轭先验通常并不比其他方法好，主要原因是现代计算方法允许我们使用几乎任何先验进行推断，而不仅仅是那些在数学上方便的有限选择。尽管如此，共轭先验在学习贝叶斯推断时、以及在某些需要对后验使用解析表达式的情况下，可能仍然非常有用（ 参阅 <a class="reference internal" href="chp_10.html#conjugate-case-study"><span class="std std-ref">10.2.2 示例：近实时推断</span></a> 中的示例 ）。因此，我们会简要地讨论 <code class="docutils literal notranslate"><span class="pre">贝塔-二项式（</span> <span class="pre">Beta-Binomial</span> <span class="pre">）模型</span></code>中的解析共轭先验。顾名思义，该模型的似然为二项分布，其共轭先验为贝塔分布：</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid Y) \propto \overbrace{\frac{N!}{y!(N-y)!} \theta^y (1 - \theta)^{N-y}}^{\text{binomial-likelihood}} \: \overbrace{\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, \theta^{\alpha-1}(1-\theta)^{\beta-1}}^{\text{beta.prior}} \]</div>
<p>式中所有不包含 <span class="math notranslate nohighlight">\(\theta\)</span> 的项都是关于 <span class="math notranslate nohighlight">\(\theta\)</span> 不变的，可以省略它们，进而得到：</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid Y) \propto \overbrace{\theta^y (1 - \theta)^{N-y}}^{\text{binomial-likelihood}} \: \overbrace{ \theta^{\alpha-1}(1-\theta)^{\beta-1}}^{\text{beta.prior}}\]</div>
<p>重新组织公式，得到：</p>
<div class="math notranslate nohighlight" id="equation-eq-kernel-beta">
<span class="eqno">(9)<a class="headerlink" href="#equation-eq-kernel-beta" title="Permalink to this equation">¶</a></span>\[p(\theta \mid Y) \propto \theta^{\alpha-1+y}(1-\theta)^{\beta-1+N-y} \]</div>
<p>如果想确保后验是一个正确的概率分布函数，我们需要添加一个归一化常数，以确保密度函数的积分为 <span class="math notranslate nohighlight">\(1\)</span>（参见 <a class="reference internal" href="chp_11.html#cont-rvs"><span class="std std-ref">连续型随机变量及其分布</span></a> ）。请注意，公式 <a class="reference internal" href="#equation-eq-kernel-beta">(9)</a> 看起来像贝塔分布的核，由此添加贝塔分布的归一化常数后，得出 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code> 的后验分布为：</p>
<div class="math notranslate nohighlight" id="equation-eq-beta-posterior">
<span class="eqno">(10)<a class="headerlink" href="#equation-eq-beta-posterior" title="Permalink to this equation">¶</a></span>\[p(\theta \mid Y) \propto \frac{\Gamma(\alpha_{post}+\beta_{post})}{\Gamma(\alpha_{post})\Gamma(\beta_{post})} \theta^{\alpha_{post}-1}(1-\theta)^{\beta_{post}-1} = \text{Beta}(\alpha_{post}, \beta_{post}) \]</div>
<p>其中 <span class="math notranslate nohighlight">\(\alpha_{post} = \alpha+y\)</span> 和 <span class="math notranslate nohighlight">\(\beta_{post} = \beta+N-y\)</span>。</p>
<p>由于 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code>的后验也是贝塔分布，我们可以使用其作为下一步贝叶斯分析的先验。这意味着，<em>一次使用整个数据集</em> 和 <em>一次使用一个数据点</em> 将获得相同的结果。例如，<a class="reference internal" href="#fig-beta-binomial-update"><span class="std std-numref">Fig. 7</span></a> 的前四个子图显示了从 <span class="math notranslate nohighlight">\(0\)</span> 到 <span class="math notranslate nohighlight">\(1\)</span>、<span class="math notranslate nohighlight">\(2\)</span> 和 <span class="math notranslate nohighlight">\(3\)</span> 试验时，不同的先验是如何更新的。如果遵循此顺序，或者如果从 <span class="math notranslate nohighlight">\(0\)</span> 次试验跳到 <span class="math notranslate nohighlight">\(3\)</span> 次试验（ 或者，实际上是 <span class="math notranslate nohighlight">\(n\)</span> 次试验 ），得到的结果是相同的。</p>
<p>从 <a class="reference internal" href="#fig-beta-binomial-update"><span class="std std-numref">Fig. 7</span></a> 中还可以看到很多其他有趣的事情。例如，随着试验次数增加，后验的宽度越来越小，即不确定性越来越低。子图 <span class="math notranslate nohighlight">\(3\)</span> 和 <span class="math notranslate nohighlight">\(5\)</span> 显示了 <span class="math notranslate nohighlight">\(2\)</span> 次试验成功 <span class="math notranslate nohighlight">\(1\)</span> 次和 <span class="math notranslate nohighlight">\(12\)</span> 次试验成功 <span class="math notranslate nohighlight">\(6\)</span> 次的结果。两种情况的采样比例估计 <span class="math notranslate nohighlight">\(\hat \theta = \frac{y}{n}\)</span>（ 黑点 ）相同，均为 <span class="math notranslate nohighlight">\(0.5\)</span>（ 后验分布的众数也是 <span class="math notranslate nohighlight">\(0.5\)</span> ），不过子图 5 中中的后验宽度相对聚集，这反映了观测数量更大，不确定性更低。最后，我们可以观察到：随着观测次数增加，不同先验最终可以收敛到相同的后验分布。在无限数据的情况下，后验与用于计算它的先验无关，不同先验得到的后验最终将在 <span class="math notranslate nohighlight">\(\hat \theta = \frac{y}{n}\)</span> 处具有所有密度。</p>
<div class="literal-block-wrapper docutils container" id="binomial-update">
<div class="code-block-caption"><span class="caption-number">Listing 7 </span><span class="caption-text">binomial_update</span><a class="headerlink" href="#binomial-update" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.ravel.html#numpy.ravel" title="numpy.ravel"><span class="n">np</span><span class="o">.</span><span class="n">ravel</span></a><span class="p">(</span><span class="n">axes</span><span class="p">)</span>

<span class="n">n_trials</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">180</span><span class="p">]</span>
<span class="n">success</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">59</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">success</span><span class="p">)</span>

<span class="n">beta_params</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="n">θ</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1500</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">s_n</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"s"</span> <span class="k">if</span> <span class="p">(</span><span class="n">N</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">else</span> <span class="s2">""</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">jdx</span><span class="p">,</span> <span class="p">(</span><span class="n">a_prior</span><span class="p">,</span> <span class="n">b_prior</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">beta_params</span><span class="p">):</span>
        <span class="n">p_theta_given_y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">a_prior</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">b_prior</span> <span class="o">+</span> <span class="n">N</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>

        <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">p_theta_given_y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">viridish</span><span class="p">[</span><span class="n">jdx</span><span class="p">])</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.divide.html#numpy.divide" title="numpy.divide"><span class="n">np</span><span class="o">.</span><span class="n">divide</span></a><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">"o"</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">N</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2"> trial</span><span class="si">{</span><span class="n">s_n</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">y</span><span class="si">:</span><span class="s2">4d</span><span class="si">}</span><span class="s2"> success"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-beta-binomial-update">
<a class="reference internal image-reference" href="../_images/beta_binomial_update.png"><img alt="../_images/beta_binomial_update.png" src="../_images/beta_binomial_update.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">从 <span class="math notranslate nohighlight">\(3\)</span> 个不同的先验开始连续更新先验并增加试验次数。黑点代表采样比例的估计 <span class="math notranslate nohighlight">\(\hat \theta = \frac{y}{n}\)</span>。</span><a class="headerlink" href="#fig-beta-binomial-update" title="Permalink to this image">¶</a></p>
</div>
<p>贝塔分布的均值是 <span class="math notranslate nohighlight">\(\frac{\alpha}{\alpha + \beta}\)</span> ，因此先验均值是：</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\theta]  = \frac{\alpha}{\alpha + \beta}\]</div>
<p>后验均值为：</p>
<div class="math notranslate nohighlight" id="equation-eq-beta-binom-mean">
<span class="eqno">(11)<a class="headerlink" href="#equation-eq-beta-binom-mean" title="Permalink to this equation">¶</a></span>\[\mathbb{E}[\theta \mid Y]  = \frac{\alpha + y}{\alpha + \beta + n}\]</div>
<p>可以看到，如果 <span class="math notranslate nohighlight">\(n\)</span> 的值相对于 <span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 的值较小，那么后验均值将更接近于先验均值。也就是说，先验对结果的贡献大于数据。如果出现相反的情况，则后验均值将更接近采样比例的估计 <span class="math notranslate nohighlight">\(\hat \theta = \frac{y}{n}\)</span> ，实际上在 <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span> 的情况下，后验均值将与 <span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 的先验无关，最终都完美匹配样本比例。</p>
<p>对于 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code>，后验众数为：</p>
<div class="math notranslate nohighlight" id="equation-eq-beta-binom-mode">
<span class="eqno">(12)<a class="headerlink" href="#equation-eq-beta-binom-mode" title="Permalink to this equation">¶</a></span>\[\operatorname*{argmax}_{\theta}{[\theta \mid Y]}  = \frac{\alpha + y - 1}{\alpha + \beta + n - 2}\]</div>
<p>可以看到，当先验为 <span class="math notranslate nohighlight">\(\text{Beta}(\alpha\!=\!1, \beta\!=\!1)\)</span> ( 即均匀分布 ) 时，后验众数在数值上等价于采样比例的估计 <span class="math notranslate nohighlight">\(\hat \theta = \frac{y}{n}\)</span>。后验众数通常被称为<strong>最大后验( MAP )</strong>  值。此结果并非 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span> <span class="pre">模型</span></code>独有。事实上，许多非贝叶斯方法的结果都可以被理解为贝叶斯方法在特定先验条件下的最大后验 <a class="footnote-reference brackets" href="#id58" id="id31">15</a>。</p>
<p>将公式 <a class="reference internal" href="#equation-eq-beta-binom-mean">(11)</a> 与采样比例 <span class="math notranslate nohighlight">\(\frac{y}{n}\)</span> 进行比较。贝叶斯估计器将 <span class="math notranslate nohighlight">\(\alpha\)</span> 添加到成功次数，将 <span class="math notranslate nohighlight">\(\alpha + \beta\)</span> 添加到试验次数。这使得 <span class="math notranslate nohighlight">\(\beta\)</span> 成为失败的次数。从此意义上说，我们可以将先验参数视为<em>伪计数</em>。先验 <span class="math notranslate nohighlight">\(\text{Beta}(1, 1)\)</span> 等价于进行两次试验，<span class="math notranslate nohighlight">\(1\)</span> 次成功，<span class="math notranslate nohighlight">\(1\)</span> 次失败。从概念上讲，贝塔分布的形状由参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 控制，观测数据会更新先验，以便使贝塔分布的形状更接近、更窄地移动到大多数观测值。对于 <span class="math notranslate nohighlight">\(\alpha &lt; 1\)</span> 和/或 <span class="math notranslate nohighlight">\(\beta &lt; 1\)</span> 的值，先验的解释变得有点奇怪，因为字面解释会说先验 <span class="math notranslate nohighlight">\(\text{Beta}(0.5, 0.5)\)</span> 对应于一次试验，半次失败，半次成功，或者可能是一次结果未定的试验。诡异！</p>
</div>
<div class="section" id="objective-priors">
<span id="id32"></span><h3>1.4.2 客观先验<a class="headerlink" href="#objective-priors" title="Permalink to this headline">¶</a></h3>
<p>在没有先验信息的情况下，遵循 <em>无差别原则</em> 听起来似乎更合理。此原则基本上是说，如果你没有关于某个问题的信息，那么你没有任何理由相信一个结果会比任何其他结果更有可能发生。</p>
<p>在贝叶斯统计背景下，这一原则激发了**客观先验（ Objective Priors ）**的研究和应用。这些是生成“对给定分析影响最小的先验”的系统方法。有些统计学者偏爱客观先验，因为此类先验消除了先验的主观性。但其实这并没有消除其他来源的主观性，例如：似然的选择、数据的选择、问题的选择等等。</p>
<p>获得客观先验的一种方法是 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span> <span class="pre">(</span> <span class="pre">JP</span> <span class="pre">)</span></code>。此类先验通常被称为<em>非信息性</em>，即便其总是以某种方式提供了信息。更好的描述是说： <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span> <span class="pre">(</span> <span class="pre">JP</span> <span class="pre">)</span></code>具有在<strong>重参数化</strong>下保持不变性的性质，即以不同但在数学上等效的方式编写表达式。让我们用一个例子来解释这究竟意味着什么。</p>
<p>假设 <code class="docutils literal notranslate"><span class="pre">Alice</span></code> 具有参数为 <span class="math notranslate nohighlight">\(\theta\)</span> 的二项似然，她选择某种先验并计算后验。而她的朋友 <code class="docutils literal notranslate"><span class="pre">Bob</span></code>，对同一问题感兴趣，但不关心成功的次数 <span class="math notranslate nohighlight">\(\theta\)</span> ，而关心成功的<strong>赔率（ odds ）</strong>，即 <span class="math notranslate nohighlight">\(\kappa\)</span> ，<span class="math notranslate nohighlight">\(\kappa = \frac{\ theta}{1-\theta}\)</span> 。 <code class="docutils literal notranslate"><span class="pre">Bob</span></code> 有两种选择：一是使用 <code class="docutils literal notranslate"><span class="pre">Alice</span></code> 在 <span class="math notranslate nohighlight">\(\theta\)</span> 上的后验来计算 <span class="math notranslate nohighlight">\(\kappa\)</span> <a class="footnote-reference brackets" href="#id59" id="id33">16</a> ，二是选择 <span class="math notranslate nohighlight">\(\kappa\)</span> 上的先验来计算自己的后验。 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 保证：如果 <code class="docutils literal notranslate"><span class="pre">Alice</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Bob</span></code> 都使用了 Jeffreys 先验，那么无论 <code class="docutils literal notranslate"><span class="pre">Bob</span></code> 做出哪种选择，都会得到相同的结果。从此意义上说，最终结果相对于所选择的参数具有不变性。此解释的一个推论是，除非使用 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code>，否则无法保证模型的两种（或更多）参数化必然会导致一致的后验。</p>
<p>对于一维 <span class="math notranslate nohighlight">\(\theta\)</span> 的情况， <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 为：</p>
<div class="math notranslate nohighlight" id="equation-eq-jeffreys-prior0">
<span class="eqno">(13)<a class="headerlink" href="#equation-eq-jeffreys-prior0" title="Permalink to this equation">¶</a></span>\[p(\theta) \propto \sqrt{I(\theta)} \]</div>
<p>其中 <span class="math notranslate nohighlight">\(I(\theta)\)</span> 为预期 <code class="docutils literal notranslate"><span class="pre">Fisher</span> <span class="pre">信息</span></code>：</p>
<div class="math notranslate nohighlight" id="equation-eq-jeffreys-prior">
<span class="eqno">(14)<a class="headerlink" href="#equation-eq-jeffreys-prior" title="Permalink to this equation">¶</a></span>\[I(\theta) = - \mathbb{E_{Y}}\left[\frac{d^2}{d\theta^2} \log p(Y \mid \theta)\right] \]</div>
<p>一旦实践者确定了似然函数 <span class="math notranslate nohighlight">\(p(Y \mid \theta)\)</span> ，那么 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 就会自动确定，从而消除了对先前选择的任何讨论。</p>
<p>有关 <code class="docutils literal notranslate"><span class="pre">Alice</span></code> 和 <code class="docutils literal notranslate"><span class="pre">Bob</span></code> 问题 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 的详细推导，请参阅 <a class="reference internal" href="chp_11.html#jeffreys-prior-derivation"><span class="std std-ref">11.6 Jeffreys 先验的推导</span></a> 。</p>
<p>如果想在这里跳过细节，则 <code class="docutils literal notranslate"><span class="pre">Alice</span></code> 的 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 为：</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 p(\theta) \propto \theta^{-0.5} (1-\theta)^{-0.5}
\end{aligned}\]</div>
<p>这变成了 <span class="math notranslate nohighlight">\(\text{Beta}(0.5, 0.5)\)</span> 分布的核。这是一个 <span class="math notranslate nohighlight">\(U\)</span> 形分布，如 <a class="reference internal" href="#fig-jeffrey-priors"><span class="std std-numref">Fig. 8</span></a> 的左上子图所示。</p>
<p>对于 <code class="docutils literal notranslate"><span class="pre">Bob</span></code> 来说， <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 为：</p>
<div class="math notranslate nohighlight" id="equation-fig-bob-prior">
<span class="eqno">(15)<a class="headerlink" href="#equation-fig-bob-prior" title="Permalink to this equation">¶</a></span>\[p(\kappa) \propto \kappa^{-0.5} (1 + \kappa)^{-1}\]</div>
<p>这是一个半 <span class="math notranslate nohighlight">\(U\)</span> 形分布，定义在 <span class="math notranslate nohighlight">\([0, \infty)\)</span> 区间中，参见 <a class="reference internal" href="#fig-jeffrey-priors"><span class="std std-numref">Fig. 8</span></a> 中的右上角子图。称其为半 <span class="math notranslate nohighlight">\(U\)</span> 形可能有点奇怪，但实际上，它是贝塔分布的近亲，即参数为 <span class="math notranslate nohighlight">\(\alpha=\beta=0.5\)</span> 时的 <code class="docutils literal notranslate"><span class="pre">Beta-prime</span> <span class="pre">分布</span></code> 核。</p>
<div class="figure align-default" id="fig-jeffrey-priors">
<a class="reference internal image-reference" href="../_images/Jeffrey_priors.png"><img alt="../_images/Jeffrey_priors.png" src="../_images/Jeffrey_priors.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">上图：根据成功次数 <span class="math notranslate nohighlight">\(\theta\)</span>（ 左 ）或几率 <span class="math notranslate nohighlight">\(\kappa\)</span>（ 右 ）参数化的二项似然的 Jeffreys 先验（ 未归一化 ）。下图：根据成功次数 <span class="math notranslate nohighlight">\(\theta\)</span>（ 左 ）或几率 <span class="math notranslate nohighlight">\(\kappa\)</span>（ 右 ）参数化的二项似然的 Jeffreys 后验（ 未归一化 ）。后验之间的箭头表示，通过应用变量规则变化，后验之间可相互转换（ 详细信息参阅 <a class="reference internal" href="chp_11.html#transformations"><span class="std std-ref">变换</span></a> ）。</span><a class="headerlink" href="#fig-jeffrey-priors" title="Permalink to this image">¶</a></p>
</div>
<p>请注意，公式 <a class="reference internal" href="#equation-eq-jeffreys-prior">(14)</a> 中的期望是关于 <span class="math notranslate nohighlight">\(Y \mid \theta\)</span> 的，这是在样本空间上的期望。这意味着为了获得 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code>，我们需要对所有可能的实验结果求平均，这违反了似然原则 <a class="footnote-reference brackets" href="#id60" id="id34">17</a>，因为关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的推断不仅取决于手头数据，还取决于潜在（但尚未）观测的数据集。</p>
<p><code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 可以是不恰当的先验，即它可能不会积分为 <span class="math notranslate nohighlight">\(1\)</span> 。例如，已知方差的高斯分布，其均值参数的 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 在整个实数轴上均匀分布。但只要我们能够验证，这些不恰当的先验与似然组合后，能够产生恰当（ 即积分为 <span class="math notranslate nohighlight">\(1\)</span> ）的后验，则这些不恰当的先验就是可以使用的。</p>
<p>另外还要注意，我们不能从不恰当的先验中抽取随机样本（ 即此类先验是非生成式的 ），这会造成许多能够帮助我们做模型推断的工具失效。</p>
<p><code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 并不是获得客观先验的唯一方法。另一种途径是通过最大化先验和后验之间的<code class="docutils literal notranslate"><span class="pre">预期</span> <span class="pre">KL</span> <span class="pre">散度</span></code>来获得先验（ 参见 <a class="reference internal" href="chp_11.html#dkl"><span class="std std-ref">11.3 KL 散度</span></a> ）。此类先验被称为 <code class="docutils literal notranslate"><span class="pre">Bernardo</span> <span class="pre">参考先验</span></code>，之所以是客观性的，是因为这些先验是 “允许数据将最大量信息带入后验” 的先验。 <code class="docutils literal notranslate"><span class="pre">Bernardo</span> <span class="pre">参考先验</span></code>和 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 不必一致。</p>
<p>此外，对于某些复杂模型，可能不存在客观先验或难以推导出客观先验。</p>
</div>
<div class="section" id="maximum-entropy-priors">
<span id="id35"></span><h3>1.4.3 最大熵先验<a class="headerlink" href="#maximum-entropy-priors" title="Permalink to this headline">¶</a></h3>
<p>另一种证明先验选择合理性的方法是选择具有最大熵的先验。此时，如果我们对参数可取的合理值完全无区别对待，那么此先验的结果就是合理值范围内的均匀分布 <a class="footnote-reference brackets" href="#id61" id="id36">18</a>。但是，当我们对可取的合理值并非无动于衷呢？例如，我们可能知道参数仅限于 <span class="math notranslate nohighlight">\([0, \infty)\)</span> 区间。我们能否得到具有最大熵同时满足给定约束的先验？是的，这正是最大熵先验背后的思想。在文献中，当人们谈论最大熵原理时，通常会找到 <code class="docutils literal notranslate"><span class="pre">MaxEnt</span></code> 这个词。</p>
<p>为了获得最大熵先验，需要求解一个包含一组约束条件的优化问题。从数学上讲，这可以使用<code class="docutils literal notranslate"><span class="pre">拉格朗日乘数</span></code>来实现。我们会使用几个代码示例来感性认识它，而不是采用形式化的证明。</p>
<p><a class="reference internal" href="#fig-max-entropy"><span class="std std-numref">Fig. 9</span></a> 展示了通过最大化熵获得的 <span class="math notranslate nohighlight">\(3\)</span> 个分布。紫色分布是在没有约束条件下获得的，我们会发现这确实是 <a class="reference internal" href="chp_11.html#entropy"><span class="std std-ref">11.2 熵</span></a> 中所预期的均匀分布。如果我们对问题一无所知，那么所有事件都是同样可能的。第二个青色的分布是在知道分布均值（ 本例中为 <span class="math notranslate nohighlight">\(1.5\)</span> ）的约束下获得的，这是一个类似指数的分布。最后一个黄绿色的分布是在已知 <span class="math notranslate nohighlight">\(3\)</span> 和 <span class="math notranslate nohighlight">\(4\)</span> 的出现概率为 <span class="math notranslate nohighlight">\(0.8\)</span> 这个约束下获得的。</p>
<blockquote>
<div><p>注意：如果检查代码 <a class="reference internal" href="#max-ent-priors"><span class="std std-ref">max_ent_priors</span></a>，你会看到所有分布都是在两个基本约束条件下计算的，一是概率只能在 <span class="math notranslate nohighlight">\([0, 1]\)</span> 区间中取值，二是总概率必须为 <span class="math notranslate nohighlight">\(1\)</span> 。由于它们是有效概率分布的通用约束，因此可以被视为固有约束。出于此原因，我们经常默认它们，进而称 <a class="reference internal" href="#fig-max-entropy"><span class="std std-numref">Fig. 9</span></a> 中的紫色分布是在无约束条件下获得的。</p>
</div></blockquote>
<div class="literal-block-wrapper docutils container" id="max-ent-priors">
<div class="code-block-caption"><span class="caption-number">Listing 8 </span><span class="caption-text">max_ent_priors</span><a class="headerlink" href="#max-ent-priors" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cons</span> <span class="o">=</span> <span class="p">[[{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"eq"</span><span class="p">,</span> <span class="s2">"fun"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html#numpy.sum" title="numpy.sum"><span class="n">np</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">}],</span>
        <span class="p">[{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"eq"</span><span class="p">,</span> <span class="s2">"fun"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html#numpy.sum" title="numpy.sum"><span class="n">np</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">},</span>
         <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"eq"</span><span class="p">,</span> <span class="s2">"fun"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.5</span> <span class="o">-</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html#numpy.sum" title="numpy.sum"><span class="n">np</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">))}],</span>
        <span class="p">[{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"eq"</span><span class="p">,</span> <span class="s2">"fun"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html#numpy.sum" title="numpy.sum"><span class="n">np</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">},</span>
         <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"eq"</span><span class="p">,</span> <span class="s2">"fun"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html#numpy.sum" title="numpy.sum"><span class="n">np</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">x</span><span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span> <span class="o">-</span> <span class="mf">0.8</span><span class="p">}]]</span>

<span class="n">max_ent</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cons</span><span class="p">):</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">entropy</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x0</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">]</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span>
                   <span class="n">constraints</span><span class="o">=</span><span class="n">c</span><span class="p">)[</span><span class="s1">'x'</span><span class="p">]</span>
    <span class="n">max_ent</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="p">(</span><span class="n">val</span><span class="p">))</span>
    <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot" title="matplotlib.pyplot.plot"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span></a><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">val</span><span class="p">,</span> <span class="s1">'o--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">viridish</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html#matplotlib.pyplot.xlabel" title="matplotlib.pyplot.xlabel"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span></a><span class="p">(</span><span class="s2">"$t$"</span><span class="p">)</span>
<a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html#matplotlib.pyplot.ylabel" title="matplotlib.pyplot.ylabel"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span></a><span class="p">(</span><span class="s2">"$p(t)$"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-max-entropy">
<a class="reference internal image-reference" href="../_images/max_entropy.png"><img alt="../_images/max_entropy.png" src="../_images/max_entropy.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">在不同约束下通过最大化熵获得的离散分布。我们使用 <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> 的函数 <code class="docutils literal notranslate"><span class="pre">entropy</span></code> 来估计这些分布。请注意添加约束极大地改变了分布。</span><a class="headerlink" href="#fig-max-entropy" title="Permalink to this image">¶</a></p>
</div>
<p>我们可以将最大熵原理视为<strong>在给定约束下选择最平坦分布的过程</strong>，在贝叶斯统计中扩展为<strong>在给定约束下选择最平坦先验分布的过程</strong>。在 <a class="reference internal" href="#fig-max-entropy"><span class="std std-numref">Fig. 9</span></a> 中，均匀分布是最平坦的分布，但请注意，一旦引入 “ <span class="math notranslate nohighlight">\(3\)</span> 和 <span class="math notranslate nohighlight">\(4\)</span> 的出现概率为 80% ” 的约束，绿色分布就变成了最平坦的分布。</p>
<p>请注意本例中 <span class="math notranslate nohighlight">\(3\)</span> 和 <span class="math notranslate nohighlight">\(4\)</span> 的出现概率都是 <span class="math notranslate nohighlight">\(0.4\)</span> ，尽管要获得 <span class="math notranslate nohighlight">\(0.8\)</span> 的目标值，有很多种选择，例如： <span class="math notranslate nohighlight">\(0+0.8\)</span> 、 <span class="math notranslate nohighlight">\(0.7+0.1\)</span> 、 <span class="math notranslate nohighlight">\(0.312+0.488\)</span> 等。另请注意，值 <span class="math notranslate nohighlight">\(1\)</span> 、 <span class="math notranslate nohighlight">\(2 、 \)</span>5<span class="math notranslate nohighlight">\( 和 \)</span>6<span class="math notranslate nohighlight">\( 也有类似情况，它们的总概率为 \)</span>0.2<span class="math notranslate nohighlight">\( ，而本例中也仅使用了均匀分布（ 每个值的概率均为 \)</span>0.05$ ）。</p>
<p>现在看一下类似指数的曲线，它看起来肯定不是很平坦，但再次注意到其他选择将更不平坦且更集中，例如，分别以 <span class="math notranslate nohighlight">\(50\%\)</span> 的概率获得 <span class="math notranslate nohighlight">\(1\)</span> 和 <span class="math notranslate nohighlight">\(2\)</span>（ 因此 <span class="math notranslate nohighlight">\(3\)</span> 至 <span class="math notranslate nohighlight">\(6\)</span> 的概率为 <span class="math notranslate nohighlight">\(0\)</span> ），这也将有 <span class="math notranslate nohighlight">\(1.5\)</span> 的期望值。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ite</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">entropies</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros" title="numpy.zeros"><span class="n">np</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="n">ite</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ite</span><span class="p">):</span>
    <span class="n">rnds</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros" title="numpy.zeros"><span class="n">np</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">x_</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html#numpy.random.choice" title="numpy.random.choice"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span></a><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">rnd</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html#numpy.random.uniform" title="numpy.random.uniform"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">total</span><span class="p">)</span>
        <span class="n">rnds</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">rnd</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">rnds</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">rnds</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rnds</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">entropy</span><span class="p">(</span><span class="n">rnds</span><span class="p">)</span>
    <span class="n">entropies</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">-</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html#numpy.sum" title="numpy.sum"><span class="n">np</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(</span><span class="n">rnds</span> <span class="o">*</span> <span class="n">x_</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.01</span><span class="p">:</span>
        <span class="n">entropies</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>
    <span class="n">prob_34</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">rnds</span><span class="p">[</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html#numpy.argwhere" title="numpy.argwhere"><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span></a><span class="p">((</span><span class="n">x_</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">x_</span> <span class="o">==</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">-</span> <span class="n">prob_34</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.01</span><span class="p">:</span>
        <span class="n">entropies</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-max-entropy-vs-random-dist"><span class="std std-numref">Fig. 10</span></a> 显示了在与 <a class="reference internal" href="#fig-max-entropy"><span class="std std-numref">Fig. 9</span></a> 中 <span class="math notranslate nohighlight">\(3\)</span> 个分布完全相同条件下，为随机生成的样本计算的熵的分布。垂直虚线表示 <a class="reference internal" href="#fig-max-entropy-vs-random-dist"><span class="std std-numref">Fig. 10</span></a> 中曲线的熵。虽然这不是一个证明，但实验似乎表明没有分布会比 <a class="reference internal" href="#fig-max-entropy-vs-random-dist"><span class="std std-numref">Fig. 10</span></a> 中的分布具有更高的熵，这与理论告诉我们的完全一致。</p>
<div class="figure align-default" id="fig-max-entropy-vs-random-dist">
<a class="reference internal image-reference" href="../_images/max_entropy_vs_random_dist.png"><img alt="../_images/max_entropy_vs_random_dist.png" src="../_images/max_entropy_vs_random_dist.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">一组随机生成的分布的熵的分布。垂直虚线表示具有最大熵分布的值，使用代码 <a class="reference internal" href="#max-ent-priors"><span class="std std-ref">max_ent_priors</span></a> 计算。可以看到，没有一个随机生成的分布的熵大于具有最大熵分布的熵，尽管这不是形式化的证明，但结果是令人放心的。</span><a class="headerlink" href="#fig-max-entropy-vs-random-dist" title="Permalink to this image">¶</a></p>
</div>
<p>在特定约束下，具有最大熵的分布封闭是 <a class="footnote-reference brackets" href="#id62" id="id37">19</a>：</p>
<ul class="simple">
<li><p>无约束时：均匀分布（连续的或离散的，取决于随机变量类型）</p></li>
<li><p>正均值，支持 <span class="math notranslate nohighlight">\([0, \infty)\)</span> ：指数分布</p></li>
<li><p>绝对值均值，支持 <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span>：拉普拉斯（也称为双指数）</p></li>
<li><p>给定均值和方差，支持 <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span>：正态分布</p></li>
<li><p>给定均值和方差，支持 <span class="math notranslate nohighlight">\([-\pi, \pi]\)</span>：Von Mises</p></li>
<li><p>只有两个无序结果和一个恒定均值：二项分布，或者存在罕见事件的泊松分布（ 泊松可以看作二项分布的特例 ）</p></li>
</ul>
<p>有趣的是，考虑到模型约束，许多传统上的广义线性模型（ 如 <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">第 3 章</span></a> 中描述的模型 ）都是使用最大熵分布来定义的。与客观先验类似，<code class="docutils literal notranslate"><span class="pre">MaxEnt</span> <span class="pre">先验</span></code>可能不存在或难以推导。</p>
</div>
<div class="section" id="weakly-informative-priors-and-regularization-priors">
<span id="id38"></span><h3>1.4.4 弱信息性先验和正则化先验<a class="headerlink" href="#weakly-informative-priors-and-regularization-priors" title="Permalink to this headline">¶</a></h3>
<p>在前面部分中，我们使用一般性过程来生成模糊的、无信息的先验，旨在不将<em>太多</em>信息放入分析中。这些一般性过程还提供了“以某种方式”自动生成先验的方法。这两点特征听起来非常具有吸引力，而且在实际上被大量贝叶斯实践者和理论家所采用。</p>
<p>但在本书中，我们不会过分依赖这些先验。我们认为先验选择应该取决于上下文，这意味着来自特定问题的细节甚至给定科学领域的特质，都可以为选择先验提供信息。虽然 <code class="docutils literal notranslate"><span class="pre">MaxEnt</span> <span class="pre">先验</span></code>能够包含其中一些约束，但我们还可以更加靠近信息性先验频谱的信息端，我们可以用弱信息先验来实现这一点。</p>
<p>构造弱信息先验的方法通常在数学上没有像 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 或 <code class="docutils literal notranslate"><span class="pre">MaxEnt</span> <span class="pre">先验</span></code> 那样的良好定义。相反，它们更多是<em>经验主义的</em>和<em>模型驱动的</em>，也就是说，它们是通过领域专业知识和模型本身的组合来定义的。对于很多问题，我们经常有关于参数可取值的信息，这些信息往往来自于参数的物理意义，例如身高必须是正数。我们甚至可以从以前实验或观测中得到参数的合理取值范围。我们或许有充分理由证明一个值应该接近零或高于某个预定义的下界。我们可以使用这些信息来为分析提供微弱的信息，同时保持一定程度的无知。</p>
<p>再次使用 <code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span></code> 示例，<a class="reference internal" href="#fig-prior-informativeness-spectrum"><span class="std std-numref">Fig. 11</span></a> 显示了四个可选先验。其中两个是 <code class="docutils literal notranslate"><span class="pre">Jeffreys</span> <span class="pre">先验</span></code> 和最大熵先验。另外一种是弱信息先验，它优先考虑 <span class="math notranslate nohighlight">\(\theta=0.5\)</span> 的值，同时保持对其他值很宽泛或相对模糊。最后一个是信息丰富的先验，以 <span class="math notranslate nohighlight">\(\theta=0.8\)</span> 为中心 <a class="footnote-reference brackets" href="#id63" id="id39">20</a> 。如果从理论、之前的实验、观测数据等中能够获得高质量信息，则信息性先验是一个有效的选择。信息性先验可以传达大量信息，因此它们通常需要比其他先验更强的理由。正如 <code class="docutils literal notranslate"><span class="pre">Carl</span> <span class="pre">Sagan</span></code> 常说的 “非凡主张需要非凡的证据” <span id="id40">[<a class="reference internal" href="references.html#id100">12</a>]</span>。重要的：先验的信息量取决于模型和模型上下文。一个在某种情境中的无信息先验，可能在另一个情境中变得非常有用 <span id="id41">[<a class="reference internal" href="references.html#id63">13</a>]</span> 。例如，如果以米为单位对成年人的平均身高进行建模，则 <span class="math notranslate nohighlight">\(\mathcal{N}(2,1)\)</span> 的先验可以被认为是无信息的，但如果估计长颈鹿的高度，则该先验变得信息量非常大，因为现实中的长颈鹿的高度与人类身高相差很大。</p>
<div class="figure align-default" id="fig-prior-informativeness-spectrum">
<a class="reference internal image-reference" href="../_images/prior_informativeness_spectrum.png"><img alt="../_images/prior_informativeness_spectrum.png" src="../_images/prior_informativeness_spectrum.png" style="width: 8.00in;"/></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">先验信息谱：虽然 <code class="docutils literal notranslate"><span class="pre">Jeffrey</span> <span class="pre">先验</span></code>和 <code class="docutils literal notranslate"><span class="pre">MaxEnt</span> <span class="pre">先验</span></code>是为二项似然唯一定义的，但弱信息先验和信息性先验不是，而是取决于之前的信息和实践者的建模决策。</span><a class="headerlink" href="#fig-prior-informativeness-spectrum" title="Permalink to this image">¶</a></p>
</div>
<p>因为弱信息先验可以将后验分布保持在一定的合理范围内，所以它们也被称为正则化先验。正则化是一种添加信息的过程，目的是解决不适定问题或减少过拟合的机会，先验提供了一种执行正则化的原则方法。</p>
<p>在本书中，我们经常使用弱信息先验。有时会在没有太多理由的情况下在模型中使用先验，仅仅是因为示例的重点可能与贝叶斯建模工作流程的其他方面有关。但我们也会展示一些使用先验预测检查来校准先验分布的例子。</p>
<div class="admonition-overfitting admonition">
<p class="admonition-title">Overfitting</p>
<p>当模型生成的预测非常接近用于拟合它的有限数据集时，就会发生过拟合，但它无法拟合新数据和/或不能很好地预测未来的观测结果。也就是说，它未能将其预测推广到更广泛的可观测结果。过拟合的对应物是欠拟合，即模型未能充分捕捉数据的底层结构。我们将在 <a class="reference internal" href="chp_02.html#model-cmp"><span class="std std-ref">2.5 模型比较</span></a> 和 <a class="reference internal" href="chp_11.html#information-criterion"><span class="std std-ref">11.4 信息准则</span></a> 部分讨论此主题的更多信息。</p>
</div>
</div>
<div class="section" id="using-prior-predictive-distributions-to-assess-priors">
<span id="id42"></span><h3>1.4.5 使用先验预测分布评估先验<a class="headerlink" href="#using-prior-predictive-distributions-to-assess-priors" title="Permalink to this headline">¶</a></h3>
<p>在评估先验选择时，<a class="reference internal" href="#automating-inference"><span class="std std-ref">1.3 人工建模与自动推断</span></a> 中显示的先验预测分布是一个方便的工具。</p>
<p>通过从先验预测分布中采样，计算机将<em>在参数空间中的选择</em>转换为<em>在观测空间中的样本</em>。考虑观测值通常比考虑模型参数更容易，这使模型评估变得更容易。遵循<code class="docutils literal notranslate"><span class="pre">Beta-Binomial</span></code> 模型，而不是判断 <span class="math notranslate nohighlight">\(\theta\)</span> 的特定值是否合理，先验预测分布允许我们判断特定数量的成功是否合理。这对于参数通过许多数学运算或多个先验相互交互的复杂模型更加有用。</p>
<p>最后，计算先验预测分布可以帮助我们确保模型已经正确编写，并且能够在概率编程语言中运行，甚至可以帮助我们调试模型。</p>
<p>在接下来的章节中，我们将看到更具体的示例，说明如何推断先验预测样本并使用它们来选择合理的先验。</p>
</div>
</div>
<div class="section" id="exercises1">
<span id="id43"></span><h2>1.5 练习<a class="headerlink" href="#exercises1" title="Permalink to this headline">¶</a></h2>
<p>Problems are labeled Easy (E), Medium (M), and Hard (H).</p>
<p><strong>1E1.</strong> As we discussed, models are artificial representations used to help define and understand an object or process.</p>
<p>However, no model is able to perfectly replicate what it represents and thus is deficient in some way. In this book we focus on a particular type of models, statistical models. What are other types of models you can think of? How do they aid understanding of the thing that is being modeled? How are they deficient?</p>
<p><strong>1E2.</strong> Match each of these verbal descriptions to their corresponding mathematical expression:</p>
<ol class="simple">
<li><p>The probability of a parameter given the observed data</p></li>
<li><p>The distribution of parameters before seeing any data</p></li>
<li><p>The plausibility of the observed data given a parameter value</p></li>
<li><p>The probability of an unseen observation given the observed data</p></li>
<li><p>The probability of an unseen observation before seeing any data</p></li>
</ol>
<p><strong>1E3.</strong> From the following expressions, which one corresponds to the sentence, The probability of being sunny given that it is July 9th of 1816?</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(p(\text{sunny})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\text{sunny} \mid \text{July})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\text{sunny} \mid \text{July 9th of 1816})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\text{July 9th of 1816} \mid \text{sunny})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(\text{sunny}, \text{July 9th of 1816}) / p(\text{July 9th of 1816})\)</span></p></li>
</ol>
<p><strong>1E4.</strong> Show that the probability of choosing a human at random and picking the Pope is not the same as the probability of the Pope being human. In the animated series Futurama, the (Space) Pope is a reptile. How does this change your previous calculations?</p>
<p><strong>1E5.</strong> Sketch what the distribution of possible observed values could be for the following cases:</p>
<ol class="simple">
<li><p>The number of people visiting your local cafe assuming Poisson   distribution</p></li>
<li><p>The weight of adult dogs in kilograms assuming a Uniform   distribution</p></li>
<li><p>The weight of adult elephants in kilograms assuming Normal   distribution</p></li>
<li><p>The weight of adult humans in pounds assuming skew Normal   distribution</p></li>
</ol>
<p><strong>1E6.</strong> For each example in the previous exercise, use SciPy to specify the distribution in Python. Pick parameters that you believe are reasonable, take a random sample of size 1000, and plot the resulting distribution. Does this distribution look reasonable given your domain knowledge? If not adjust the parameters and repeat the process until they seem reasonable.</p>
<p><strong>1E7.</strong> Compare priors <span class="math notranslate nohighlight">\(\text{Beta}(0.5, 0.5)\)</span>, <span class="math notranslate nohighlight">\(\text{Beta}(1, 1)\)</span>, <span class="math notranslate nohighlight">\(\text{Beta}(1, 4)\)</span>. How do the priors differ in terms of shape?</p>
<p><strong>1E8</strong>. Rerun Code Block <a class="reference internal" href="#binomial-update"><span class="std std-ref">binomial_update</span></a> but using two Beta-priors of your choice. Hint: you may what to try priors with <span class="math notranslate nohighlight">\(\alpha \neq \beta\)</span> like <span class="math notranslate nohighlight">\(\text{Beta}(2, 5)\)</span>.</p>
<p><strong>1E9.</strong> Try to come up with new constraints in order to obtain new Max-Ent distributions (Code Block <a class="reference internal" href="#max-ent-priors"><span class="std std-ref">max_ent_priors</span></a>)</p>
<p><strong>1E10.</strong> In Code Block <a class="reference internal" href="#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a>, change the value of <code class="docutils literal notranslate"><span class="pre">can_sd</span></code> and run the Metropolis-Hastings sampler. Try values like 0.001 and 1.</p>
<ol class="simple">
<li><p>Compute the mean, SD, and HDI and compare the values with those in   the book (computed using <code class="docutils literal notranslate"><span class="pre">can_sd=0.05</span></code>). How different are the   estimates?</p></li>
<li><p>Use the function <code class="docutils literal notranslate"><span class="pre">az.plot_posterior</span></code>.</p></li>
</ol>
<p><strong>1E11.</strong> You need to estimate the weights of blue whales, humans, and mice. You assume they are normally distributed, and you set the same prior <span class="math notranslate nohighlight">\(\mathcal{HN}(200\text{kg})\)</span> for the variance. What type of prior is this for adult blue whales? Strongly informative, weakly informative, or non-informative? What about for mice and for humans? How does informativeness of the prior correspond to our real world intuitions about these animals?</p>
<p><strong>1E12.</strong> Use the following function to explore different combinations of priors (change the parameters <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>) and data (change heads and trials). Summarize your observations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">posterior_grid</span><span class="p">(</span><span class="n">grid</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html#scipy.stats.beta" title="scipy.stats.beta"><span class="n">stats</span><span class="o">.</span><span class="n">beta</span></a><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
    <span class="n">posterior</span> <span class="o">/=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"heads = </span><span class="si">{</span><span class="n">heads</span><span class="si">}</span><span class="se">\n</span><span class="s2">trials = </span><span class="si">{</span><span class="n">trials</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">e_n</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
            <span class="p">[</span><span class="n">prior</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">posterior</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">"prior"</span><span class="p">,</span> <span class="s2">"likelihood"</span><span class="p">,</span> <span class="s2">"posterior"</span><span class="p">])):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="s2">"o-"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">e_n</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>


<span class="n">interact</span><span class="p">(</span><span class="n">posterior_grid</span><span class="p">,</span>
    <span class="n">grid</span><span class="o">=</span><span class="n">ipyw</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">15</span><span class="p">),</span>
    <span class="n">a</span><span class="o">=</span><span class="n">ipyw</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">b</span><span class="o">=</span><span class="n">ipyw</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">heads</span><span class="o">=</span><span class="n">ipyw</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">6</span><span class="p">),</span>
    <span class="n">trials</span><span class="o">=</span><span class="n">ipyw</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">9</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>1E13.</strong> Between the prior, prior predictive, posterior, and posterior predictive distributions which distribution would help answer each of these questions. Some items may have multiple answers.</p>
<ol class="simple">
<li><p>How do we think is the distribution of parameters values before   seeing any data?</p></li>
<li><p>What observed values do we think we could see before seeing any   data?</p></li>
<li><p>After estimating parameters using a model what do we predict we will   observe next?</p></li>
<li><p>What parameter values explain the observed data after conditioning   on that data?</p></li>
<li><p>Which can be used to calculate numerical summaries, such as the   mean, of the parameters?</p></li>
<li><p>Which can can be used to to visualize a Highest Density Interval?</p></li>
</ol>
<p><strong>1M14.</strong> Equation <a class="reference internal" href="#equation-eq-posterior-dist">(1)</a> contains the marginal likelihood in the denominator, which is difficult to calculate.</p>
<p>In Equation <a class="reference internal" href="#equation-eq-proportional-bayes">(3)</a> we show that knowing the posterior up to a proportional constant is sufficient for inference.</p>
<p>Show why the marginal likelihood is not needed for the Metropolis-Hasting method to work. Hint: this is a pen and paper exercise, try by expanding Equation <a class="reference internal" href="#equation-acceptance-prob">(7)</a>.</p>
<p><strong>1M15.</strong> In the following definition of a probabilistic model, identify the prior, the likelihood, and the posterior:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split} 
Y \sim \mathcal{N}(\mu, \sigma)\\ 
\mu \sim \mathcal{N}(0, 1)\\ 
\sigma \sim \mathcal{HN}(1)\\ 
\end{split}\end{split}\]</div>
<p><strong>1M16.</strong> In the previous model, how many parameters will the posterior have? Compare your answer with that from the model in the coin-flipping problem in Equation <a class="reference internal" href="#equation-eq-beta-binomial">(8)</a>.</p>
<p><strong>1M17.</strong> Suppose that we have two coins; when we toss the first coin, half of the time it lands tails and half of the time on heads. The other coin is a loaded coin that always lands on heads. If we choose one of the coins at random and observe a head, what is the probability that this coin is the loaded one?</p>
<p><strong>1M18.</strong> Modify Code Block <a class="reference internal" href="#metropolis-hastings-sampler-rvs"><span class="std std-ref">metropolis_hastings_sampler_rvs</span></a> to generate random samples from a Poisson distribution with parameters of your choosing.</p>
<p>Then modify Code Blocks <a class="reference internal" href="#metropolis-hastings-sampler"><span class="std std-ref">metropolis_hastings_sampler</span></a> and <a class="reference internal" href="#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> to generate MCMC samples estimating your chosen parameters. Test how the number of samples, MCMC iterations, and initial starting point affect convergence to your true chosen parameter.</p>
<p><strong>1M19.</strong> Assume we are building a model to estimate the mean and standard deviation of adult human heights in centimeters. Build a model that will make these estimation. Start with Code Block <a class="reference internal" href="#beta-binom"><span class="std std-ref">beta_binom</span></a> and change the likelihood and priors as needed. After doing so then</p>
<ol class="simple">
<li><p>Sample from the prior predictive. Generate a visualization and   numerical summary of the prior predictive distribution</p></li>
<li><p>Using the outputs from (a) to justify your choices of priors and   likelihoods</p></li>
</ol>
<p><strong>1M20.</strong> From domain knowledge you have that a given parameter can not be negative, and has a mean that is roughly between 3 and 10 units, and a standard deviation of around 2. Determine two prior distribution that satisfy these constraints using Python. This may require trial and error by drawing samples and verifying these criteria have been met using both plots and numerical summaries.</p>
<p><strong>1M21.</strong> A store is visited by <span class="math notranslate nohighlight">\(n\)</span> customers on a given day.</p>
<p>The number of customers that make a purchase <span class="math notranslate nohighlight">\(Y\)</span> is distributed as <span class="math notranslate nohighlight">\(\text{Bin}(n, \theta)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the probability that a customer makes a purchase. Assume we know <span class="math notranslate nohighlight">\(\theta\)</span> and the prior for <span class="math notranslate nohighlight">\(n\)</span> is <span class="math notranslate nohighlight">\(\text{Pois}(4.5)\)</span>.</p>
<ol class="simple">
<li><p>Use PyMC3 to compute the posterior distribution of <span class="math notranslate nohighlight">\(n\)</span> for all   combinations of <span class="math notranslate nohighlight">\(Y \in {0, 5, 10}\)</span> and <span class="math notranslate nohighlight">\(\theta \in {0.2, 0.5}\)</span>. Use   <code class="docutils literal notranslate"><span class="pre">az.plot_posterior</span></code> to plot the results in a single plot.</p></li>
<li><p>Summarize the effect of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> on the posterior</p></li>
</ol>
<p><strong>1H22.</strong> Modify Code Block <a class="reference internal" href="#metropolis-hastings-sampler-rvs"><span class="std std-ref">metropolis_hastings_sampler_rvs</span></a> to generate samples from a Normal Distribution, noting your choice of parameters for the mean and standard deviation. Then modify Code Blocks <a class="reference internal" href="#metropolis-hastings-sampler"><span class="std std-ref">metropolis_hastings_sampler</span></a> and <a class="reference internal" href="#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> to sample from a Normal model and see if you can recover your chosen parameters.</p>
<p><strong>1H23.</strong> Make a model that estimates the proportion of the number of sunny versus cloudy days in your area. Use the past 5 days of data from your personal observations. Think through the data collection process. How hard is it to remember the past 5 days. What if needed the past 30 days of data? Past year? Justify your choice of priors. Obtain a posterior distribution that estimates the proportion of sunny versus cloudy days. Generate predictions for the next 10 days of weather.</p>
<p>Communicate your answer using both numerical summaries and visualizations.</p>
<p><strong>1H24.</strong> You planted 12 seedlings and 3 germinate. Let us call <span class="math notranslate nohighlight">\(\theta\)</span> the probability that a seedling germinates. Assuming <span class="math notranslate nohighlight">\(\text{Beta}(1, 1)\)</span> prior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<ol class="simple">
<li><p>Use pen and paper to compute the posterior mean and standard   deviation. Verify your calculations using SciPy.</p></li>
<li><p>Use SciPy to compute the equal-tailed and highest density <span class="math notranslate nohighlight">\(94\%\)</span>   posterior intervals.</p></li>
<li><p>Use SciPy to compute the posterior predictive probability that at   least one seedling will germinate if you plant another 12 seedlings.</p></li>
</ol>
<p>After obtaining your results with SciPy repeat this exercise using PyMC3 and ArviZ</p>
<hr class="footnotes docutils"/>
<dl class="footnote brackets">
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id5">1</a></span></dt>
<dd><p>If you want to be more general you can even say that everything is   a probability distribution as a quantity you assume to know with   arbitrary precision that can be described by a Dirac delta function.</p>
</dd>
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id6">2</a></span></dt>
<dd><p>Some authors call these quantities latent variables and reserve   the name parameter to identify fixed, but unknown, quantities.</p>
</dd>
<dt class="label" id="id46"><span class="brackets"><a class="fn-backref" href="#id8">3</a></span></dt>
<dd><p>Alternatively you can think of this in terms of certainty or   information, depending if you are a glass half empty or glass half   full person.</p>
</dd>
<dt class="label" id="id47"><span class="brackets"><a class="fn-backref" href="#id9">4</a></span></dt>
<dd><p>Sometimes the word <em>distribution</em> will be implicit, this commonly occurs when discussing these topics.</p>
</dd>
<dt class="label" id="id48"><span class="brackets"><a class="fn-backref" href="#id10">5</a></span></dt>
<dd><p>Here we are using experiment in the broad sense of any procedure   to collect or generate data.</p>
</dd>
<dt class="label" id="id49"><span class="brackets"><a class="fn-backref" href="#id11">6</a></span></dt>
<dd><p><a class="reference external" href="https://xkcd.com/2117/">https://xkcd.com/2117/</a></p>
</dd>
<dt class="label" id="id50"><span class="brackets"><a class="fn-backref" href="#id12">7</a></span></dt>
<dd><p>Technically we should talk about the expectation of a random   variable. See Section <a class="reference internal" href="chp_11.html#expectations"><span class="std std-ref">期望</span></a> for details.</p>
</dd>
<dt class="label" id="id51"><span class="brackets"><a class="fn-backref" href="#id15">8</a></span></dt>
<dd><p>See detailed balance at Sections <a class="reference internal" href="chp_11.html#markov-chains"><span class="std std-ref">马尔可夫链</span></a> and <a class="reference internal" href="chp_11.html#sec-metropolis-hastings"><span class="std std-ref">Metropolis-Hastings 采样器</span></a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets"><a class="fn-backref" href="#id17">9</a></span></dt>
<dd><p>For a more extensive discussion about inference methods you should   read Section [<a class="reference internal" href="chp_11.html#inference-methods"><span class="std std-ref">11.9 推断方法</span></a> and references   therein.</p>
</dd>
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id18">10</a></span></dt>
<dd><p>This is sometimes referred to as a kernel in other Universal   Inference Engines.</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id19">11</a></span></dt>
<dd><p>You can use ArviZ <code class="docutils literal notranslate"><span class="pre">plot_trace</span></code> function to get a similar plot.  This is how we will do in the rest of the book.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id21">12</a></span></dt>
<dd><p>Notice that in principle the number of possible intervals   containing a given proportion of the total density is infinite.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id29">13</a></span></dt>
<dd><p>For more examples check   <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions</a></p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id30">14</a></span></dt>
<dd><p>Except, the ones happening in your brain.</p>
</dd>
<dt class="label" id="id58"><span class="brackets"><a class="fn-backref" href="#id31">15</a></span></dt>
<dd><p>For example, a regularized linear regression with a L2   regularization is the same as using a Gaussian prior on the   coefficient.</p>
</dd>
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id33">16</a></span></dt>
<dd><p>For example, if we have samples from the posterior, then we can   plug those samples of <span class="math notranslate nohighlight">\(\theta\)</span> into   <span class="math notranslate nohighlight">\(\kappa = \frac{\theta}{1-\theta}\)</span>.</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id34">17</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Likelihood_principle">https://en.wikipedia.org/wiki/Likelihood_principle</a></p>
</dd>
<dt class="label" id="id61"><span class="brackets"><a class="fn-backref" href="#id36">18</a></span></dt>
<dd><p>See Section <a class="reference internal" href="chp_11.html#entropy"><span class="std std-ref">11.2 熵</span></a> for more details.</p>
</dd>
<dt class="label" id="id62"><span class="brackets"><a class="fn-backref" href="#id37">19</a></span></dt>
<dd><p>Wikipedia has a longer list at   <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Other_examples">https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Other_examples</a></p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id39">20</a></span></dt>
<dd><p>Even when the definition of such priors will require more context   than the one provided, we still think the example conveys a useful   intuition, that will be refined as we progress through this book.</p>
</dd>
</dl>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./zh_CN"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="symbollist.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">符号表</p>
</div>
</a>
<a class="right-next" href="chp_02.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">第二章: 贝叶斯模型的探索性分析</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</div>
</div>
<footer class="footer">
<p>
    
      By Martin, Kumar, Lao<br/>
    
        © Copyright 2021.<br/>
</p>
</footer>
</main>
</div>
</div>
<script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
</body>
</html>