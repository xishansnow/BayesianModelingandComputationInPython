{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de48d0ee",
   "metadata": {},
   "source": [
    "(chap3_5)= \n",
    "\n",
    "# 第五章: 样条 \n",
    "\n",
    "<style>p{text-indent:2em;2}</style>\n",
    "\n",
    "在本章中，我们将讨论样条曲线，它是第 [3](chap2) 章中概念的扩展，旨在增加更多灵活性。在第 [3](chap2) 章中介绍的模型中，结果变量和预测变量之间的关系在整个域中是相同的。相比之下，样条曲线可以将一个问题分解为多个局部解决方案，所有这些局部解决方案可以组合起来产生一个有用的全局解决方案。\n",
    "\n",
    "(polynomial-regression)= \n",
    "\n",
    "## 5.1 多项式回归 \n",
    "\n",
    "正如在第 [3](chap2) 章中看到的，我们可以将线性模型写为：\n",
    "\n",
    "```{math} \n",
    ":label: eq:lin_model\n",
    "\n",
    "\\mathbb{E}[Y]= \\beta_0 + \\beta_1 X\n",
    "\n",
    "```\n",
    "\n",
    "其中 $\\beta_0$ 是截距，$\\beta_1$ 是斜率，$\\mathbb{E}[Y]$ 是结果（随机）变量 $Y$ 的期望值或均值。我们可以将公式 {eq}`eq:lin_model` 改写为以下形式：\n",
    "\n",
    "```{math} \n",
    ":label: eq:polynomial_reg\n",
    "\n",
    "\\mathbb{E}[Y]= \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\cdots + \\beta_m X^m\n",
    "\n",
    "```\n",
    "\n",
    "上式被称为多项式回归。\n",
    "\n",
    "乍一看，公式 {eq}`eq:polynomial_reg` 似乎表示预测变量 $X、X^2 \\cdots + X^m$ 的多元线性回归。从某种意义上说这也没有错，但关键是所有预测变量 $X^m$ 都是从 $X$ 的 $1$ 到 $m$ 整数幂派生而来。因此，就实际问题而言，我们仍然在拟合一元预测变量。\n",
    "\n",
    "我们称 $m$ 为多项式的度。第 [3](chap2) 章和 [4](chap3) 章的线性回归模型都是 $1$ 次多项式。唯一的例外是 {ref}`transforming_covariates` 节中的变方差示例，其中使用了 $m=1/2$。\n",
    "\n",
    "{numref}`fig:polynomial_regression` 显示了 $3$ 个使用 $2$、$10$ 和 $15$ 次的多项式回归示例。随着我们增加多项式的阶数，我们会得到更灵活的曲线。\n",
    "\n",
    "```{figure} figures/polynomial_regression.png\n",
    ":name: fig:polynomial_regression\n",
    ":width: 8.00in\n",
    "\n",
    "度为 $2$、$10$ 和 $15$ 的多项式回归示例。随着度数增加，拟合变得更加摇摆。虚线是删除用蓝色十字表示的观测值后的拟合。当多项式的次数为 $2$ 或 $10$ 时，删除数据点的影响较小，但在次数为 $15$ 时效果比较明显。使用最小二乘法计算拟合。\n",
    "\n",
    "``` \n",
    "\n",
    "多项式的缺陷之一是其全局性，当我们应用一个度为 $m$ 的多项式时，其实是在说：“预测变量和结果变量之间的关系对于整个数据集的度是 $m$ ”。当数据的不同区域需要不同级别的灵活性时，这会出现问题，比如在某些区域导致曲线过于灵活 [^1]。在 {numref}`fig:polynomial_regression`  的最后一个度为 $15$ 子图中，可以看到，在 $X$ 值增大的过程中，拟合曲线呈现出一个深谷，然后是一个高峰，即便不存在具有如此低或高值的真实观测数据点。\n",
    "\n",
    "此外，随着度数增加，拟合变得更倾向于那些被删除的点，或者等效于添加了若干未来数据；换句话说，随着度数的增加，模型变得比较容易过拟合。例如，在 {numref}`fig:polynomial_regression` 中，黑线表示对整个数据集的拟合，虚线表示删除一个数据点（图中用叉号表示）后的拟合。可以看到，尤其是在最后一个子图中，即使删除单个数据点也会改变模型的拟合结果，这种效应甚至延伸至了远离该点的位置。\n",
    "\n",
    "(expanding_feature_space)= \n",
    "\n",
    "## 5.2 扩展特征空间 \n",
    "\n",
    "在概念层面上，我们可以将多项式回归视为创建新预测变量的药方，或者表述为更正式的术语：**对特征空间进行扩展**。通过执行特征扩展，我们在扩展空间中拟合的一条直线，可能代表原始数据空间中的一条曲线，非常简洁！然而，特征扩展并不是随意使用的，我们不能总是期望通过对数据使用随机的变换，就能得到好的结果。\n",
    "\n",
    "事实上，正如我们刚刚看到的那样，应用多项式并非没有问题。\n",
    "\n",
    "为了概括特征扩展的概念，除了多项式模型，我们可以将公式 {eq}`eq:lin_model` 扩展为以下形式：\n",
    "\n",
    "```{math} \n",
    ":label: eq:bfr\n",
    "\n",
    "\\mathbb{E}[Y]= \\beta_0 + \\beta_1 B_{1}(X_{1}) + \\beta_2 B_{2}(X_{2}) + \\cdots + \\beta_m B_{m}(X_{m})\n",
    "\n",
    "```\n",
    "\n",
    "其中 $B_i$ 是任意函数，我们称之为基函数。基函数的线性组合为我们提供了一个函数 $f$，它才是我们实际看到的拟合。从这个意义上说，$B_i$ 是构建灵活函数 $f$ 的幕后技巧。\n",
    "\n",
    "```{math} \n",
    ":label: eq:bfr2 \n",
    "\n",
    "\\mathbb{E}[Y]= \\sum_i^m \\beta_i B_{i}(X_{i}) = f(X) \n",
    "\n",
    "``` \n",
    "\n",
    "基函数 $B_i$ 有很多选择，多项式是其中之一，也可以应用任意一组函数，例如 $2$ 的幂、对数或平方根等。选择这些函数可能是由待解决的问题驱动的，例如，在第 {ref}`transforming_covariates` 节中，我们通过计算平方根来模拟婴儿的身高随年龄变化的函数，其动机是人类婴儿在生命早期阶段生长得更快，然后趋于平稳，类似于平方根函数的效果。\n",
    "\n",
    "另一种替代方法是使用 $I(c_i \\leq x_k < c_j)$ 之类的指示函数，将原始 $\\boldsymbol{X}$ 预测变量分解为（非重叠的）子集。然后仅在这些子集内拟合局部的多项式。此过程导致拟合出 **分段多项式** [^2]，如 {numref}`fig:piecewise` 所示。\n",
    "\n",
    "```{figure} figures/piecewise.png\n",
    ":name: fig:piecewise\n",
    ":width: 8.00in\n",
    "\n",
    "蓝线是我们试图逼近的 *真实* 函数。黑色实线是递增阶数（$1$、$2$、$3$ 和 $4$）的分段多项式。在 $x$ 轴上垂直的灰色虚线标记每个子域的约束边界。\n",
    "\n",
    "``` \n",
    "\n",
    "{numref}`fig:piecewise` 的四个子图目标相同，用于近似蓝色曲线对应的函数。我们首先将函数分成 $3$ 个子域，由灰色虚线分隔，然后为每个子域拟合不同的函数。\n",
    "\n",
    "在第一个子图（ 分段常数 ）中，我们拟合了一个常数函数，可以将其视为零次多项式。聚合的解决方案，即黑色的 3 段称为 **step-function**。这似乎是一个相当粗略的近似值，但它可能就是我们所需要的。例如，如果我们试图找出不连续的结果，例如早上、下午和晚上的预期平均温度，则阶跃函数可能没问题。或者，即使我们认为结果是平滑的 [^3]，我们也可以得到一个非平滑的近似值。\n",
    "\n",
    "在第二个子图（分段线性）中，我们执行与第一个子图相同的操作，但我们使用线性函数而不是常数函数，它是一次多项式。请注意，连续线性解决方案在虚线处相遇，这是故意完成的。我们可以证明这种限制是为了使解决方案尽可能平滑[^4]。\n",
    "\n",
    "在第三个子图（分段二次）和第四个子图（分段三次）中，我们使用二次和三次分段多项式。正如我们所看到的，通过增加分段多项式的次数，我们得到了越来越灵活的解决方案，这带来了更好的拟合，但也有更高的过拟合机会。\n",
    "\n",
    "因为最终拟合是由局部解（$B_i$ 基函数）构造的函数 $f$，我们可以更轻松地适应模型的灵活性以适应不同区域的数据需求。在这种特殊情况下，我们可以使用更简单的函数（阶数较低的多项式）来拟合不同区域的数据，同时提供适合整个数据域的良好整体模型。\n",
    "\n",
    "到目前为止，我们假设我们有一个预测变量 $X$，但同样的想法可以扩展到多个预测变量 $X_0, X_1, \\cdots, X_p$ (注意：$X_i$ 之间相互独立，见公式)。我们甚至可以添加一个反向链接函数 $\\phi$ [^5] 这种形式的模型被称为广义可加模型 (GAM)：{cite:p}`GelmanBayesianDataAnalysis2013, WoodGeneralizedAdditiveModels2017`。 \n",
    "\n",
    "```{math} \n",
    ":label: eq:GAM \n",
    "\n",
    "\\mathbb{E}[Y]= \\phi \\left(\\sum_i^p f(X_i)\\right) \n",
    "\n",
    "``` \n",
    "\n",
    "概括一下在本节中学到的知识，公式 {eq}`eq:bfr` 中的 $B_i$ 函数是一种巧妙的统计工具，它允许我们拟合更灵活的模型。原则上，可以自由选择任意 $B_i$ 函数，根据领域知识做探索性数据分析，并形成阶段性结果，甚至可以通过反复试验来选择 $B_i$ 。并非所有转换都具有相同的统计属性，因此最好能够在更广泛的数据集上评估一些具有良好通用属性的默认函数。从下一节开始，本书将讨论限制在被称为 `基样条` 的基函数族 [^6] 。\n",
    "\n",
    "(introducing-splines)= \n",
    "\n",
    "## 5.3 样条简介 \n",
    "\n",
    "样条曲线可以看作是试图利用多项式的灵活性、同时又能控制其缺点的一种具有整体良好统计特性的模型。要定义样条曲线，首先要定义结点（ Knots ） [^7]。结点的作用是将变量 $\\boldsymbol{X}$ 的域分割成连续区间。例如，{numref}`fig:piecewise` 中的灰色垂直虚线就表示结点。为了达到既具备灵活性，又能控制多项式缺陷的目的，样条曲线被设计为一个连续的分段多项式，也就是说，我们强制要求两个连续区间的子多项式在结点处相遇。如果子多项式的度数 ( Degree ) 为 $n$，则称该样条曲线的度数为 $n$。有时也用样条曲线的阶数（ Order ，增加一项幂为 $0$ 的常数项）来表示，即相同情况下，阶数为度数加 $1$ 。\n",
    "\n",
    "在 {numref}`fig:piecewise` 中，可以看到随着分段多项式阶数增加，结果函数的*平滑度*也会增加。正如已经提到的，子多项式应该在结点处相遇。在第一个子图上，我们似乎在作弊，因为每行之间有一个台阶，并不连续，但如果限制条件是在每个区间使用常数，那么这可能是最好的结果了。\n",
    "\n",
    "在谈论样条时，子多项式被形式化地称为基样条或简称 `B-样条`。给定次数的任何样条函数都可以构造为该次数基样条的线性组合。 \n",
    "\n",
    "{numref}`fig:splines_basis` 显示了从 $0$ 到 $3$（从上到下）递增程度的基样条示例，底部的点代表结点，其中蓝色的点表示基样条 (黑色实线）值不为 $0$ 的结点（即该基样条的作用域）。为清楚起见，所有其他基样条均用较细的虚线表示，但所有基样条都同样重要。事实上，{numref}`fig:splines_basis` 中的每个子图都显示了由给定结点定义的所有基样条。换句话说，基样条完全由一组结点和一个度数定义。\n",
    "\n",
    "```{figure} figures/splines_basis.png\n",
    ":name: fig:splines_basis\n",
    ":width: 8.00in\n",
    "\n",
    "逐步增加度数的基样条，从 $0$ 到 $3$。顶部子图为一个阶梯函数，第二个子图三角函数，然后是越来越多的高斯函数。图中添加了边界处的堆叠结点（用较小黑点表示），以便能够定义靠近边界处的样条。\n",
    "\n",
    "``` \n",
    "\n",
    "从 {numref}`fig:splines_basis` 中可以看到，随着基样条度数的增加，基样条的域跨度也越来越大 [^8]。因此，为了使更高度数的样条有意义，需要定义更多结点。请注意，在所有情况下，基样条仅在给定区间内被限制为非零。这个属性使样条回归比多项式回归更具备*局部性*。\n",
    "\n",
    "控制每个基样条的结点数随着度数增加而增长，对于所有大于 $0$ 的度数，我们无法在边界附近定义基样条。这就是当增加度数时，基样条曲线在边界 {numref}`fig:splines_basis` 处以黑色突出显示的原因。这带来了一个潜在问题，因为它在边界处的基样条较少，所以我们的近似会在那里受到影响。幸运的是，这个边界问题很容易解决，我们只需要在边界处添加结点（参见 {numref}`fig:splines_basis` 中的小黑点）。因此，如果我们的结点是 $(0,1,2,3,4,5)$ 并且想要拟合三次样条曲线（就像在 {numref}`fig:splines_basis` 的最后一个子图中），那么我们实际使用节点集合应当为 $(0,0,0,0,1,2,3,4,5,5,5,5)$ 。也就是说，在起点填充了 $3$ 次结点 $0$ ，最后填充了 $3$ 次结点 $5$。通过这样做，现在有五个必要的结点 $(0,0,0,0,1)$ 来定义第一个基样条（ 参见 {numref}`fig:splines_basis`  的最后一个子图中看起来像指数分布的靛蓝色虚线 )。然后使用结点 $(0,0,0,1,2)$ 来定义第二个基样条（ 看起来像 `Beta 分布`的那个 ）等等。看看由结点 $(0,1,2,3,4)$ 定义的第一个完整基样条（以黑色突出显示）。请注意，我们需要在边界处填充与样条度数一样多的结点。这就是为什么 $0$ 度无需额外结点，而 $3$ 度需要额外 $6$ 个结点。\n",
    "\n",
    "每个单独的基样条本身并不是很有用，但是它们的线性组合能够拟合出复杂的函数。因此，在实践中拟合样条需要选择基样条顺序、结点数量以及结点位置，然后找到一组系数来加权所有基样条。这在 {numref}`fig:splines_weighted` 中表示。我们可以看到使用不同颜色表示的基函数，以帮助个性化每个单独的基函数。结点在每个子图的底部用黑点表示。第二行更有趣，因为可以看到与第一行相同基函数被一组 $\\beta_i$ 系数缩放。较粗的连续黑线表示通过基样条与 $\\beta$ 系数加权后得到的样条。\n",
    "\n",
    "```{figure} figures/splines_weighted.png\n",
    ":name: fig:splines_weighted\n",
    ":width: 8.00in\n",
    "\n",
    "使用 `Patsy` 软件包定义的基样条。在第一行中可以看到以灰色虚线表示的 1 阶（分段常数）、2 阶（分段线性）和 4 阶（分段三次）样条曲线。为了清楚起见，每个基函数都用不同的颜色表示。在第二行中，将第一行的基样条按一组系数加权，粗黑线表示了这些基函数的加权和。由于系数值是随机选择的，所以可以将第二行中的每个子图视为*样条空间*上先验分布的随机样本。\n",
    "\n",
    "``` \n",
    "\n",
    "在这个例子中，我们从半正态分布（代码 [splines_patsy_plot](splines_patsy_plot) 中的第 17 行）中采样生成 $\\beta_i$ 系数。因此，{numref}`fig:splines_weighted` 中的每个子图仅显示样条上概率分布的一种实现。通过删除随机种子，然后运行代码 [splines_patsy_plot](splines_patsy_plot) 几次就能看出这一点，每次你都会看到不同的样条曲线。此外，你还可以尝试将半正态分布替换为正态分布、指数分布等其他分布。{numref}`fig:splines_realizations` 显示了三次样条的四种实现。\n",
    "\n",
    "```{figure} figures/splines_realizations.png\n",
    ":name: fig:splines_realizations\n",
    ":width: 8.00in\n",
    "\n",
    "从半正态分布中采样得到 $\\beta_i$ 系数的四种三次样条实现。\n",
    "\n",
    "``` \n",
    "\n",
    "::: {admonition} Four is a crowd for splines \n",
    "\n",
    "在所有可能的样条中，三次样条是最常用的。\n",
    "\n",
    "但为什么三次样条曲线是样条曲线的女王呢？图 {numref}`fig:piecewise` 和 {numref}`fig:splines_weighted` 提供了一些提示。三次样条曲线为我们提供了能够为大多数场景生成*足够平滑*曲线所需的最低阶样条曲线，从而降低了更高阶样条曲线对人们的吸引力。所谓*足够平滑*是什么意思？在不深入数学细节的情况下，其大致意思是拟合的函数不会出现斜率的突然变化。这样做的一种方法是添加两个连续的分段多项式应该在其公共结点处相遇的约束条件，而三次样条另外增加了两个约束：其在结点处的一阶和二阶导数也是连续的。这意味着斜率在结点处是连续的，并且斜率的斜率也连续 [^9]。事实上，$m$ 度的样条曲线在结点处会有 $m-1$ 阶导数。\n",
    "\n",
    "说了这么多，低阶或高阶样条对于某些问题仍然有用，只是三次样条是很好的默认值。\n",
    "\n",
    "::: \n",
    "\n",
    "(building-the-design-matrix-using-patsy)= \n",
    "\n",
    "## 5.4 使用 `Patsy` 软件包构建设计矩阵\n",
    "\n",
    "在图 {numref}`fig:splines_basis` 和 {numref}`fig:splines_weighted` 中，我们绘制了基样条线，但到目前为止省略了计算它们的方法，主要原因是计算很麻烦，并且在 `Scipy` [^10] 等软件包中已经有有效的可用算法。因此，与我们不打算从头开始计算基样条，而是依赖 `Patsy` 软件包，这是一个用于描述统计模型（尤其是线性模型，或具有线性组件的模型）和构建设计矩阵的包。其灵感来自 R 编程语言生态系统的许多包中广泛使用的*公式迷你语言*。例如，具有两个预测变量的线性模型看起来像`\"y ~ x1 + x2\"`，如果想添加交互，可以写成`\"y ~ x1 + x2 + x1:x2\"`。这与第 [3](chap2) 章中突出显示 `Bambi` 语法相似。有关详细信息，请查看 `patsy` 文档 [^11]。\n",
    "\n",
    "要在 `Patsy` 中定义基样条设计矩阵，我们需要将以 `bs()` 开头的字符串*粒子*传递给 `dmatrix` 函数，该粒子是一个能够被 `Patsy` 解析为函数的字符串。因此，它可以接受多个参数，包括数据、样条结点数组、样条的度数等。在代码 [splines_patsy](splines_patsy) 中，我们定义了 $3$ 个设计矩阵，一个具有 $0$ 阶（分段常数），另一个具有 $1$ 阶（分段线性），最后一个具有 $3$ 阶（三次样条）。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: splines_patsy\n",
    ":caption: splines_patsy\n",
    "\n",
    "x = np.linspace(0., 1., 500)\n",
    "knots = [0.25, 0.5, 0.75]\n",
    "\n",
    "B0 = dmatrix(\"bs(x, knots=knots, degree=0, include_intercept=True) - 1\", \n",
    "             {\"x\": x, \"knots\":knots})\n",
    "B1 = dmatrix(\"bs(x, knots=knots, degree=1, include_intercept=True) - 1\",\n",
    "             {\"x\": x, \"knots\":knots})\n",
    "B3 = dmatrix(\"bs(x, knots=knots, degree=3,include_intercept=True) - 1\",\n",
    "             {\"x\": x, \"knots\":knots})\n",
    "```\n",
    "\n",
    "{numref}`fig:design_matrices` 表示使用代码 [splines_patsy](splines_patsy) 计算的 $3$ 个设计矩阵。为了更好地掌握 `Patsy` 正在做什么，建议你使用 Jupyter notebook/lab 或其他 IDE 来检查对象 `B0`、`B1` 和 `B2`。\n",
    "\n",
    "```{figure} figures/design_matrices.png\n",
    ":name: fig:design_matrices\n",
    ":width: 8.00in\n",
    "\n",
    "在代码 [splines_patsy](splines_patsy) 中使用 `Patsy` 生成的设计矩阵。颜色从黑色 ( $1$ ) 变为浅灰色 ( $0$ )，列数是基样条数，行数是数据点数。\n",
    "\n",
    "``` \n",
    "\n",
    "{numref}`fig:design_matrices` 的第一个子图对应于 `B0`，一个 $0$ 次样条曲线。对于前 $5$ 个观测值，我们可以看到设计矩阵是一个只有 $0$（浅灰色）和 $1$（黑色）的矩阵。第一个基样条（第 $0$ 列）为 $1$，否则为 $0$，对于前 $5$ 个观测值，第二个基样条（第 1 列）为 0，对于后 5 个观测值为 1，再次为 0。并且重复相同的模式。将此与 {numref}`fig:splines_weighted` 的第一个子图（第一行）进行比较，你应该会看到设计矩阵如何编码该图。\n",
    "\n",
    "对于 {numref}`fig:design_matrices` 中的第二个子图，我们有第一个基样条从 1 到 0，第二、第三和第四个从 0 到 1，然后从 1 到 0。第五个基样条从 0 到 1。\n",
    "\n",
    "你应该在 {numref}`fig:splines_weighted` 的第二个子图（第一行）中看到此模式如何匹配具有负斜率的线、3 个三角函数和具有正斜率的线。\n",
    "\n",
    "最后，如果我们比较 {numref}`fig:design_matrices` 的第三个子图中的 7 列如何与 {numref}`fig:splines_weighted` 的第三个子图（第一行）中的 7 条曲线相匹配，我们可以看到类似的结果。\n",
    "\n",
    "代码 [splines_patsy](splines_patsy) 用于生成图 {numref}`fig:splines_weighted` 和 {numref}`fig:design_matrices` 中的基样条，唯一不同的是前者我们使用了 `x = np .linspace(0., 1., 500)`，所以曲线看起来更平滑，我们在后面使用`x = np.linspace(0., 1., 20)`，这样矩阵更容易理解。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: splines_patsy_plot\n",
    ":caption: splines_patsy_plot\n",
    "\n",
    "_, axes = plt.subplots(2, 3, sharex=True, sharey=\"row\")\n",
    "for idx, (B, title) in enumerate(zip((B0, B1, B3),\n",
    "                                     (\"Piecewise constant\",\n",
    "                                      \"Piecewise linear\",\n",
    "                                      \"Cubic spline\"))):\n",
    "    # plot spline basis functions\n",
    "    for i in range(B.shape[1]):\n",
    "        axes[0, idx].plot(x, B[:, i],\n",
    "                          color=viridish[i], lw=2, ls=\"--\")\n",
    "    # we generate some positive random coefficients \n",
    "    # there is nothing wrong with negative values\n",
    "    β = np.abs(np.random.normal(0, 1, size=B.shape[1]))\n",
    "    # plot spline basis functions scaled by its β\n",
    "    for i in range(B.shape[1]):\n",
    "        axes[1, idx].plot(x, B[:, i]*β[i],\n",
    "                          color=viridish[i], lw=2, ls=\"--\")\n",
    "    # plot the sum of the basis functions\n",
    "    axes[1, idx].plot(x, np.dot(B, β), color=\"k\", lw=3)\n",
    "    # plot the knots\n",
    "    axes[0, idx].plot(knots, np.zeros_like(knots), \"ko\")\n",
    "    axes[1, idx].plot(knots, np.zeros_like(knots), \"ko\")\n",
    "    axes[0, idx].set_title(title)\n",
    "```\n",
    "\n",
    "到目前为止，我们已经探索了几个示例，以直观了解样条线是什么以及如何在 `Patsy` 的帮助下自动创建它们。\n",
    "\n",
    "我们现在可以继续计算权重。让我们看看如何使用 PyMC3 在贝叶斯模型中做到这一点。\n",
    "\n",
    "(fitting-splines-in-pymc3)= \n",
    "\n",
    "## 5.5 在 PyMC3 中拟合样条\n",
    "\n",
    "在本节中，我们将使用 PyMC3 通过将一组基样条拟合到数据来获得回归系数 $\\beta$ 的值。\n",
    "\n",
    "现代自行车共享系统允许全球许多城市的人们以完全自动化的方式租用和归还自行车，有助于提高公共交通的效率，并可能使社会的一部分更健康、更快乐。我们将使用来自加州大学欧文分校机器学习库 [^12] 的此类自行车共享系统的数据集。对于我们的示例，我们将估计 24 小时内每小时租用的自行车数量。让我们加载并绘制数据：\n",
    "\n",
    "```python\n",
    "data = pd.read_csv(\"../data/bikes_hour.csv\")\n",
    "data.sort_values(by=\"hour\", inplace=True)\n",
    "\n",
    "# We standardize the response variable\n",
    "data_cnt_om = data[\"count\"].mean()\n",
    "data_cnt_os = data[\"count\"].std()\n",
    "data[\"count_normalized\"] = (data[\"count\"] - data_cnt_om) / data_cnt_os\n",
    "# Remove data, you may later try to refit the model to the whole data\n",
    "data = data[::50]\n",
    "```\n",
    "\n",
    "```{figure} figures/bikes_data.png\n",
    ":name: fig:bikes_data\n",
    ":width: 8.00in\n",
    "\n",
    "自行车数据的可视化。每个点是一天中每小时租用的自行车的标准化数量（在区间 0、23 上）。这些点是半透明的，以避免点过度重叠，从而有助于查看数据的分布。\n",
    "\n",
    "``` \n",
    "\n",
    "快速查看 {numref}`fig:bikes_data` 会发现，一天中的时间与出租自行车数量之间的关系并不能通过拟合一条线来很好地捕捉到。因此，让我们尝试使用样条回归来更好地逼近非线性模式。\n",
    "\n",
    "正如我们已经提到的，为了使用样条曲线，我们需要定义结点的数量和位置。我们将使用 6 个结点并使用最简单的选项来定位它们，每个结点之间的间距相等。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: knot_list\n",
    ":caption: knot_list\n",
    "\n",
    "num_knots = 6\n",
    "knot_list = np.linspace(0, 23, num_knots+2)[1:-1]\n",
    "```\n",
    "\n",
    "请注意，在代码 [knot_list](knot_list) 中，我们定义了 8 个结点，但随后我们删除了第一个和最后一个结点，确保保留在数据的 *interior* 中定义的 6 个结点。这是否是一个有用的策略将取决于数据。例如，如果大部分数据远离边界，这将是一个好主意，结点的数量越大，它们的位置就越不重要。\n",
    "\n",
    "现在我们使用 `Patsy` 为我们定义和构建设计矩阵：\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: bikes_dmatrix\n",
    ":caption: bikes_dmatrix\n",
    "\n",
    "B = dmatrix(\n",
    "    \"bs(cnt, knots=knots, degree=3, include_intercept=True) - 1\",\n",
    "    {\"cnt\": data.hour.values, \"knots\": knot_list[1:-1]})\n",
    "```\n",
    "\n",
    "建议的统计模型是：\n",
    "\n",
    "```{math} \n",
    "\\begin{aligned}\n",
    "\\begin{split}\n",
    "    \\tau \\sim& \\; \\mathcal{HC}(1) \\\\\n",
    "    \\boldsymbol{\\beta} \\sim& \\; \\mathcal{N}(0, \\tau) \\\\\n",
    "    \\sigma \\sim& \\; \\mathcal{HN}(1) \\\\\n",
    "    Y \\sim& \\; \\mathcal{N}(\\boldsymbol{B}(X)\\boldsymbol{\\beta},\\sigma)\n",
    "\\end{split}\\end{aligned}\n",
    "```\n",
    "\n",
    "我们的样条回归模型与第  [3](chap2) 章中的线性模型非常相似。所有的工作都是由设计矩阵 $\\boldsymbol{B}$ 及其对特征空间的扩展完成的。请注意，我们使用线性代数符号将公式 {eq}`eq:bfr` 和 {eq}`eq:bfr2` 的乘法和求和写成更短的形式，即我们写成 $\\boldsymbol{\\mu} = \\boldsymbol{B}\\boldsymbol{\\beta}$ 而不是 $\\boldsymbol{\\mu} = \\sum_i^n B_i \\boldsymbol{\\beta}_i$。\n",
    "\n",
    "像往常一样，统计语法以几乎一对一的方式翻译成 PyMC3。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: splines\n",
    ":caption: splines\n",
    "\n",
    "with pm.Model() as splines:\n",
    "    τ = pm.HalfCauchy(\"τ\", 1) \n",
    "    β = pm.Normal(\"β\", mu=0, sd=τ, shape=B.shape[1])\n",
    "    μ = pm.Deterministic(\"μ\", pm.math.dot(B, β))\n",
    "    σ = pm.HalfNormal(\"σ\", 1)\n",
    "    c = pm.Normal(\"c\", μ, σ, observed=data[\"count_normalized\"].values)\n",
    "    idata_s = pm.sample(1000, return_inferencedata=True)\n",
    "```\n",
    "我们在 {numref}`fig:bikes_spline_raw_data` 中将最终拟合的线性预测显示为黑色实线，每个加权基样条显示为虚线。这是一个很好的表示，因为我们可以看到基样条如何影响最终结点果。\n",
    "\n",
    "```{figure} figures/bikes_spline_raw_data.png\n",
    ":name: fig:bikes_spline_raw_data\n",
    ":width: 8.00in\n",
    "\n",
    "使用样条拟合的自行车数据。 B样条用虚线表示。它们的总和产生更粗的黑色实线。\n",
    "绘制的值对应于后验的平均值。黑点代表结点。相对于 {numref}`fig:splines_weighted` 中绘制的样条线，此图中的样条线看起来非常*锯齿状*。原因是我们在更少的点上评估函数。此处为 24 分，因为数据每小时分箱，而 {numref}`fig:splines_weighted` 中的数据为 500。\n",
    "\n",
    "``` \n",
    "\n",
    "当我们想要显示模型的结果时，更有用的绘图是使用重叠样条及其不确定性绘制数据，如 {numref}`fig:bikes_data2`。从这个图中我们可以很容易地看出，在深夜租用自行车的数量是最低的。然后有增加，可能随着人们醒来并去工作。我们在 10 小时左右出现第一个高峰，然后趋于平稳，或者可能略有下降，然后在 18 小时左右人们通勤回家时出现第二个高峰，之后稳步下降。\n",
    "\n",
    "```{figure} figures/bikes_spline_data.png\n",
    ":name: fig:bikes_data2\n",
    ":width: 8.00in\n",
    "\n",
    "使用样条拟合的自行车数据（黑点）。阴影曲线代表 $94\\%$ HDI 区间（平均值），蓝色曲线代表平均趋势。\n",
    "\n",
    "``` \n",
    "\n",
    "在这个自行车租赁示例中，我们正在处理一个循环变量，这意味着 0 小时等于 24 小时。这对我们来说可能或多或少是显而易见的，但对于我们的模型来说绝对不明显。 `Patsy` 提供了一个简单的解决方案来告诉我们的模型变量是循环的。我们可以使用 cc 来代替使用 `bs` 定义设计矩阵，这是一个*圆形感知*的三次样条。我们建议你查看 `Patsy` 文档以获取更多详细信息，并探索在以前的模型中使用 `cc` 并比较结果。\n",
    "\n",
    "(choosing-knots-and-prior-for-splines)= \n",
    "\n",
    "## 5.6 选择样条的结点和先验 \n",
    "\n",
    "在使用样条曲线时，我们必须做出的一项建模决策是选择结点的数量和位置。这可能有点令人担忧，因为通常结点的数量和它们的间距不是明显的决定。当面对这种类型的选择时，我们总是可以尝试拟合多个模型，然后使用 LOO 等方法来帮助我们选择最佳模型。 {numref}`tab:loo_splines` 显示了拟合模型的结果，如代码 [splines](splines) 中定义的模型，具有 3、6、9、12 和 18 个等距结点。我们可以看到 LOO 选择了 12 节的样条作为最佳模型。\n",
    "\n",
    "{numref}`tab:loo_splines` 的一个有趣观察是，模型`m_12k`（排名最高的模型）的权重为 0.88，而 `m_3k`（排名最后的模型）的权重为 0.12。其余型号的重量几乎为 0。正如我们在 {ref}`model_averaging` 节中解释的，默认情况下，权重是使用堆叠计算的，这是一种尝试在元模型中组合多个模型以最小化元模型和 *true 之间的分歧的方法* 生成模型。结果，即使模型 `m_6k`、`m_9k` 和 `m_18k` 具有更好的 `loo` 值，一旦包含 `m_12k`，它们也没有太多可添加的内容，而 `m_3k` 是排名最低的模型，它似乎它仍然有一些新的东西可以为模型平均做出贡献。\n",
    "\n",
    "{numref}`fig:bikes_spline_loo_knots` 显示所有这些模型的平均拟合样条曲线。\n",
    " \n",
    "\n",
    "\n",
    "```{list-table} 使用 LOO 对具有不同结点数的样条模型进行模型比较的总结。\n",
    ":name: tab:loo_splines\n",
    "* -\n",
    "  - **rank**\n",
    "  - **loo**\n",
    "  - **p_loo**\n",
    "  - **d_loo**\n",
    "  - **weight**\n",
    "  - **se**\n",
    "  - **dse**\n",
    "  - **warning**\n",
    "  - **loo_scale**\n",
    "* -  m_12k\n",
    "  -    0\n",
    "  - -377.67\n",
    "  -   14.21\n",
    "  -    0.00\n",
    "  -    0.88\n",
    "  -   17.86\n",
    "  -    0.00\n",
    "  - False\n",
    "  - log\n",
    "* -   m_18k\n",
    "  -    1\n",
    "  - -379.78\n",
    "  -   17.56\n",
    "  -    2.10\n",
    "  -    0.00\n",
    "  -   17.89\n",
    "  -    1.45\n",
    "  - False\n",
    "  - log\n",
    "* -  m_9k\n",
    "  -    2\n",
    "  - -380.42\n",
    "  -   11.43\n",
    "  -    2.75\n",
    "  -    0.00\n",
    "  -   18.12\n",
    "  -    2.97\n",
    "  - False\n",
    "  - log\n",
    "* -  m_6k\n",
    "  -    3\n",
    "  - -389.43\n",
    "  -    9.41\n",
    "  -   11.76\n",
    "  -    0.00\n",
    "  -   18.16\n",
    "  -    5.72\n",
    "  - False\n",
    "  - log\n",
    "* - m_3k\n",
    "  -   4\n",
    "  - 400.25\n",
    "  -   7.17\n",
    "  -  22.58\n",
    "  -   0.12\n",
    "  -  18.01\n",
    "  -   7.78\n",
    "  - False\n",
    "  - log\n",
    "```\n",
    "\n",
    " \n",
    "\n",
    "```{figure} figures/bikes_spline_loo_knots.png\n",
    ":name: fig:bikes_spline_loo_knots\n",
    ":width: 8.00in\n",
    "\n",
    "具有不同结点数 (3, 6, 9, 12, 18) 的代码 [splines](splines) 中描述的模型的平均后验样条。根据 LOO，模型“m_12k”以蓝色突出显示为排名最高的模型。模型 `m_3k` 以黑色突出显示，而其余模型则显示为灰色，因为它们的权重为零（参见 {numref}`tab:loo_splines`）。\n",
    "\n",
    "``` \n",
    "\n",
    "可以帮助确定结点的位置的一条建议是根据分位数而不是统一放置它们。在代码 [knot_list](knot_list) 中，我们可以使用 `knot_list = np.quantile(data.hour, np.linspace(0, 1, num_knots))` 定义`knot_list`。通过这种方式，我们将在我们拥有更多数据的地方放置更多的结点，而在数据较少的地方放置更少的结点。这转化为数据更丰富部分的更灵活的近似值。\n",
    "\n",
    "(regularizing-prior-for-splines)= \n",
    "\n",
    "### 5.6.1 样条的正则化先验 \n",
    "\n",
    "由于选择过少的结点可能会导致欠拟合，而过多的结点可能会导致过拟合，因此我们可能希望使用*相当大*数量的结点，然后选择正则化先验。从样条线和 {numref}`fig:splines_weighted` 的定义可以看出，连续的 $\\boldsymbol{\\beta}$ 系数越接近，得到的函数就越平滑。想象一下，你在 {numref}`fig:splines_weighted` 中删除了设计矩阵的两个连续列，实际上将这些系数设置为 0，拟合将大大降低*平滑*，因为我们在预测变量中没有足够的信息来覆盖一些子区域（回想一下样条线是*本地*）。因此，我们可以通过选择 $\\boldsymbol{\\beta}$ 系数的先验来实现更平滑的拟合回归线，使得 $\\beta_{i+1}$ 的值与 $\\beta_{i}$：\n",
    "\n",
    "```{math} \n",
    "\\begin{aligned}\n",
    "\\begin{split}\n",
    "\\beta_i \\sim& \\mathcal{N}(0, 1) \\\\\n",
    "\\tau\\sim& \\mathcal{N}(0,1) \\\\\n",
    "\\beta \\sim& \\mathcal{N}(\\beta_{i-1}, \\tau) \n",
    "\\end{split}\\end{aligned}\n",
    "```\n",
    "\n",
    "使用 PyMC3，我们可以使用高斯随机游走先验分布编写等效版本：\n",
    "\n",
    "```{math} \n",
    "\\begin{aligned}\n",
    "\\begin{split}\n",
    "\\tau\\sim& \\mathcal{N}(0, 1) \\\\\n",
    "\\beta \\sim& \\mathcal{G}RW(\\beta, \\tau) \n",
    "\\end{split}\\end{aligned}\n",
    "```\n",
    "\n",
    "要查看此先验的效果，我们将重复对自行车数据集的分析，但这次使用“num_knots = 12”。我们使用“样条线”模型和以下模型重新拟合数据：\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: splines_rw\n",
    ":caption: splines_rw\n",
    "with pm.Model() as splines_rw:\n",
    "    τ = pm.HalfCauchy(\"τ\", 1) \n",
    "    β = pm.GaussianRandomWalk(\"β\", mu=0, sigma=τ, shape=B.shape[1])\n",
    "    μ = pm.Deterministic(\"μ\", pm.math.dot(B, β))\n",
    "    σ = pm.HalfNormal(\"σ\", 1)\n",
    "    c = pm.Normal(\"c\", μ, σ, observed=data[\"count_normalized\"].values)\n",
    "    trace_splines_rw = pm.sample(1000)\n",
    "```\n",
    "\n",
    "在 {numref}`fig:bikes_spline_data_grw` 上，我们可以看到模型 `splines_rw`（黑线）的样条均值函数比没有先验平滑的样条均值函数（灰色粗线）波动小，尽管我们承认差异似乎相当小。\n",
    "\n",
    "```{figure} figures/bikes_spline_data_grw.png\n",
    ":name: fig:bikes_spline_data_grw\n",
    ":width: 8.00in\n",
    "\n",
    "符合高斯先验（黑色）或正则化高斯随机游走先验（蓝色）的自行车数据。对于这两种情况，我们都使用 22 节。黑线对应于从“样条”模型计算的平均样条函数。蓝线是模型“splines_rw”的均值函数。\n",
    "\n",
    "``` \n",
    "\n",
    "(modeling-co2-uptake-with-splines)= \n",
    "\n",
    "## 5.7 用样条对 $CO_2$ 吸收量建模 \n",
    "\n",
    "对于样条曲线的最后一个示例，我们将使用来自实验研究的数据 {cite:p}`Potvin1990, Pedersen2019`。该实验包括测量 12 种不同植物在不同条件下的二氧化碳吸收量。在这里，我们将只探讨外部 CO₂ 浓度的影响，即环境中的 CO₂ 浓度如何影响不同植物对 CO₂ 的消耗。在每株植物的 7 个 CO 2 浓度下测量了 CO 2 吸收量，12 株植物中的每一种都具有相同的 7 个值。让我们从加载和整理数据开始。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: plants_co2_import\n",
    ":caption: plants_co2_import\n",
    "\n",
    "plants_CO2 = pd.read_csv(\"../data/CO2_uptake.csv\")\n",
    "plant_names = plants_CO2.Plant.unique()\n",
    "\n",
    "# Index the first 7 CO2 measurements per plant\n",
    "CO2_conc = plants_CO2.conc.values[:7]\n",
    "\n",
    "# Get full array which are the 7 measurements above repeated 12 times\n",
    "CO2_concs = plants_CO2.conc.values\n",
    "uptake = plants_CO2.uptake.values\n",
    "\n",
    "index = range(12)\n",
    "groups = len(index)\n",
    "```\n",
    "\n",
    "我们要拟合的第一个模型是具有单一结果曲线的模型，即假设所有 12 株植物的结果曲线相同。我们首先使用 `Patsy` 定义设计矩阵，就像我们之前所做的那样。\n",
    "\n",
    "我们设置 `num_knots=2` 因为我们每株植物有 7 个观察值，所以相对较少的结点数应该可以正常工作。在代码 [plants_co2_import](plants_co2_import) 中，`CO2_concs` 是一个列表，其值 `[95, 175, 250, 350, 500, 675, 1000]` 重复 12 次，每株植物一次。\n",
    "\n",
    "```python\n",
    "num_knots = 2\n",
    "knot_list = np.linspace(CO2_conc[0], CO2_conc[-1], num_knots+2)[1:-1]\n",
    "\n",
    "Bg = dmatrix(\n",
    "    \"bs(conc, knots=knots, degree=3, include_intercept=True) - 1\",\n",
    "    {\"conc\": CO2_concs, \"knots\": knot_list})\n",
    "```\n",
    "\n",
    "这个问题看起来类似于前面章节中的自行车租赁问题，因此我们可以从应用相同的模型开始。使用我们已经在以前的问题中应用过的模型或我们从文献中学到的模型是开始分析的好方法。这种模型-模板方法可以被视为模型设计过程的捷径 {cite:p}`Gelman2020`。除了不必从头开始考虑模型的明显优势之外，我们还有其他优势，例如对如何执行模型的探索性分析有更好的直觉，然后可以对模型进行更改以简化它或使它更复杂。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: sp_global\n",
    ":caption: sp_global\n",
    "\n",
    "with pm.Model() as sp_global:\n",
    "    τ = pm.HalfCauchy(\"τ\", 1)\n",
    "    β = pm.Normal(\"β\", mu=0, sigma=τ, shape=Bg.shape[1])\n",
    "    μg = pm.Deterministic(\"μg\", pm.math.dot(Bg, β))\n",
    "    σ = pm.HalfNormal(\"σ\", 1)\n",
    "    up = pm.Normal(\"up\", μg, σ, observed=uptake)\n",
    "    idata_sp_global = pm.sample(2000, return_inferencedata=True)\n",
    "```\n",
    "\n",
    "从 {numref}`fig:sp_global` 我们可以清楚地看到该模型只为某些植物提供了良好的拟合。该模型平均而言是好的，即如果我们将所有物种汇集在一起​​ ，但对于特定植物来说不是很好。\n",
    "\n",
    "```{figure} figures/sp_global.png\n",
    ":name: fig:sp_global\n",
    ":width: 8.00in\n",
    "\n",
    "黑点代表 12 种植物（Qn1、Qn2、Qn3、Qc1、Qc2、Qc3、Mn1、Mn2、Mn3、Mc1、Mc2、Mc3）中的每一种在 7 个 CO2 浓度下测量的 CO2 吸收量。黑线是代码 [sp_global](sp_global) 中模型的平均样条拟合，灰色阴影曲线表示该拟合的 $94\\%$ HDI 区间。\n",
    "\n",
    "``` \n",
    "\n",
    "现在让我们尝试使用每个工厂具有不同结果的模型，为此我们在代码 [Bi_matrix](Bi_matrix) 中定义设计矩阵“Bi”。为了定义 `Bi`，我们使用列表 `CO2_conc = [95, 175, 250, 350, 500, 675, 1000]`，因此 `Bi` 是一个 $7 \\times 7$ 矩阵，而 `Bg` 是一个 $84 \\times 7$ 矩阵。\n",
    "\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: Bi_matrix\n",
    ":caption: Bi_matrix\n",
    "\n",
    "Bi = dmatrix(\n",
    "    \"bs(conc, knots=knots, degree=3, include_intercept=True) - 1\",\n",
    "    {\"conc\": CO2_conc, \"knots\": knot_list})\n",
    "```\n",
    "\n",
    "相应地，对于 `Bi` 的形状，代码 [sp_individual](sp_individual) 中的参数 $\\beta$ 现在具有形状 `shape=(Bi.shape[1], groups))`（而不是 `shape=(Bg .shape[1]))`) 并且我们重塑`μi[:,index].T.ravel()`。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: sp_individual\n",
    ":caption: sp_individual\n",
    "\n",
    "with pm.Model() as sp_individual:\n",
    "    τ = pm.HalfCauchy(\"τ\", 1)\n",
    "    β = pm.Normal(\"β\", mu=0, sigma=τ, shape=(Bi.shape[1], groups))\n",
    "    μi = pm.Deterministic(\"μi\", pm.math.dot(Bi, β))\n",
    "    σ = pm.HalfNormal(\"σ\", 1)\n",
    "    up = pm.Normal(\"up\", μi[:,index].T.ravel(), σ, observed=uptake)\n",
    "    idata_sp_individual = pm.sample(2000, return_inferencedata=True)\n",
    "```\n",
    "\n",
    "从 {numref}`fig:sp_individual` 我们现在可以看到我们对 12 种植物中的每一种都有更好的拟合。\n",
    "\n",
    "```{figure} figures/sp_individual.png\n",
    ":name: fig:sp_individual\n",
    ":width: 8.00in\n",
    "\n",
    "在 12 种植物的 7 种 CO2 浓度下测量的 CO2 吸收量。黑线是代码 [sp_individual](sp_individual) 中模型的平均样条拟合，灰色阴影曲线表示该拟合的 $94\\%$ HDI 区间。\n",
    "\n",
    "``` \n",
    "\n",
    "我们还可以混合以前的两种模型 [^13]。如果我们想估计 12 种植物加上个体拟合的全球趋势，这可能会很有趣。代码 [sp_mix](sp_mix) 中的模型“sp_mix”使用先前定义的设计矩阵“Bg”和“Bi”。\n",
    "\n",
    "```{code-block} ipython3\n",
    ":name: sp_mix\n",
    ":caption: sp_mix\n",
    "\n",
    "with pm.Model() as sp_mix:\n",
    "    τ = pm.HalfCauchy(\"τ\", 1)\n",
    "    βg = pm.Normal(\"βg\", mu=0, sigma=τ, shape=Bg.shape[1])\n",
    "    μg = pm.Deterministic(\"μg\", pm.math.dot(Bg, βg))\n",
    "    βi = pm.Normal(\"βi\", mu=0, sigma=τ, shape=(Bi.shape[1], groups))\n",
    "    μi = pm.Deterministic(\"μi\", pm.math.dot(Bi, βi))\n",
    "    σ = pm.HalfNormal(\"σ\", 1)\n",
    "    up = pm.Normal(\"up\", μg+μi[:,index].T.ravel(), σ, observed=uptake)\n",
    "    idata_sp_mix = pm.sample(2000, return_inferencedata=True)\n",
    "```\n",
    "\n",
    "{numref}`fig:sp_mix_decomposed` 显示模型`sp_mix`的拟合。该模型的一个优点是我们可以将个体拟合（蓝色）分解为两个术语，一个全局趋势，黑色，以及每个植物的趋势偏差，灰色。注意黑色的全局趋势如何在每个子图中重复。我们可以看到，偏差不仅在平均吸收上有所不同，即它们不是平坦的直线，而且在不同程度上，它们的功能反应形状也不同。\n",
    "\n",
    "```{figure} figures/sp_mix_decomposed.png\n",
    ":name: fig:sp_mix_decomposed\n",
    ":width: 8.00in\n",
    "\n",
    "在 12 种植物的 7 种 CO2 浓度下测量的 CO2 吸收量。蓝线是代码 [sp_mix](sp_mix) 中模型的平均样条拟合，灰色阴影曲线表示该拟合的 $94\\%$ HDI 区间。这种拟合被分解为两项。黑色和深灰色带表示全局贡献，灰色和浅灰色带表示与全局贡献的偏差。蓝线和蓝带是全球趋势及其偏差的总和。\n",
    "\n",
    "``` \n",
    "\n",
    "{numref}`fig:sp_compare` 表明根据 LOO `sp_mix` 是比其他两个更好的模型。我们可以看到，由于“sp_mix”和“sp_individual”模型的标准误差部分重叠，因此该声明仍然存在一些不确定性。我们还可以看到，模型`sp_mix`和`sp_individual`比`sp_global`受到更严重的惩罚（`sp_global`的空圆圈和黑色圆圈之间的距离更短）。我们注意到 LOO 计算返回关于帕累托分布的估计形状参数大于 0.7 的警告。对于此示例，我们将在此停止，但为了进行真正的分析，我们应该进一步注意这些警告，并尝试遵循第 {ref}`k-paretto` 部分中描述的一些操作。\n",
    "\n",
    "```python\n",
    "cmp = az.compare({\"global\":idata_sp_global, \n",
    "                  \"individual\":idata_sp_individual, \n",
    "                  \"mix\":idata_sp_mix})\n",
    "```\n",
    "\n",
    "```{figure} figures/sp_compare.png\n",
    ":name: fig:sp_compare\n",
    ":width: 8.00in\n",
    "\n",
    "使用 LOO 对本章讨论的 3 种不同 CO₂ 吸收模型（`sp_global`、`sp_individual`、`sp_mix`）进行模型比较。\n",
    "\n",
    "模型的预测准确度从高到低排列。空心点代表 LOO 的值，黑点是样本内预测精度。黑色部分代表 LOO 计算的标准误差。以三角形为中心的灰色部分表示每个模型的 LOO 值与排名最佳的模型之间的差值的标准误差。\n",
    "\n",
    "``` \n",
    "\n",
    "(exercises5)= \n",
    "\n",
    "## 5.8 Exercises \n",
    "\n",
    "**5E1.**. Splines are quite powerful so its good to know when and where to use them. To reinforce this explain each of the following \n",
    "\n",
    "1.  The differences between linear regression and splines.\n",
    "\n",
    "2.  When you may want to use linear regression over splines \n",
    "\n",
    "3.  Why splines is usually preferred over polynomial regression of high     order.\n",
    "\n",
    "**5E2.** Redo {numref}`fig:polynomial_regression` but fitting a polynomial of degree 0 and of degree 1. Does they look similar to any other type of model. Hint: you may want to use the code in the GitHub repository.\n",
    "\n",
    "**5E3.** Redo {numref}`fig:piecewise` but changing the value of one or the two knots. How the position of the knots affects the fit? You will find the code in the GitHub repository.\n",
    "\n",
    "**5E4.** Below we provide some data. To each data fit a 0, 1, and 3 degree spline. Plot the fit, including the data and position of the knots. Use `knots = np.linspace(-0.8, 0.8, 4)`. Describe the fit.\n",
    "\n",
    "1.  `x = np.linspace(-1, 1., 200)` and `y = np.random.normal(2*x, 0.25)` \n",
    "\n",
    "2.  `x = np.linspace(-1, 1., 200)` and     `y = np.random.normal(x**2, 0.25)` \n",
    "\n",
    "3.  pick a function you like.\n",
    "\n",
    "**5E5.** In Code Block [bikes_dmatrix](bikes_dmatrix) we used a non-cyclic aware design matrix. Plot this design matrix. Then generate a cyclic design matrix. Plot this one too what is the difference? \n",
    "\n",
    "**5E6.** Generate the following design matrices using Patsy.\n",
    "\n",
    "```python\n",
    "x = np.linspace(0., 1., 20)\n",
    "knots = [0.25, 0.5, 0.75]\n",
    "\n",
    "B0 = dmatrix(\"bs(x, knots=knots, degree=3, include_intercept=False) +1\",\n",
    "            {\"x\": x, \"knots\":knots})\n",
    "B1 = dmatrix(\"bs(x, knots=knots, degree=3, include_intercept=True) +1\",\n",
    "            {\"x\": x, \"knots\":knots})\n",
    "B2 = dmatrix(\"bs(x, knots=knots, degree=3, include_intercept=False) -1\",\n",
    "            {\"x\": x, \"knots\":knots})\n",
    "B3 = dmatrix(\"bs(x, knots=knots, degree=3, include_intercept=True) -1\",\n",
    "            {\"x\": x, \"knots\":knots})\n",
    "```\n",
    "\n",
    "1.  What is the shape of each one of the matrices? Can you justify the     values for the shapes? \n",
    "\n",
    "2.  Could you explain what the arguments `include_intercept=True/False`     and the `+1/-1` do? Try generating figures like     {numref}`fig:splines_basis` and {numref}`fig:design_matrices` to     help you answer this question \n",
    "\n",
    "**5E7.** Refit the bike rental example using the options listed below. Visually compare the results and try to explain the results: \n",
    "\n",
    "1.  Code Block [knot_list](knot_list) but do not remove     the first and last knots (i.e. without using 1:-1) \n",
    "\n",
    "2.  Use quantiles to set the knots instead of spacing them linearly.\n",
    "\n",
    "3.  Repeat the previous two points but with less knots \n",
    "\n",
    "**5E8.** In the GitHub repository you will find the spectra dataset use it to: \n",
    "\n",
    "1.  Fit a cubic spline with knots     `np.quantile(X, np.arange(0.1, 1, 0.02))` and a Gaussian prior (like     in Code Block [splines](splines)) \n",
    "\n",
    "2.  Fit a cubic spline with knots     `np.quantile(X, np.arange(0.1, 1, 0.02))` and a Gaussian Random Walk     prior (like in Code Block [splines_rw](splines_rw)) \n",
    "\n",
    "3.  Fit a cubic spline with knots     `np.quantile(X, np.arange(0.1, 1, 0.1))` and a Gaussian prior (like     in Code Block [splines](splines)) \n",
    "\n",
    "4.  compare the fits visually and using LOO \n",
    "\n",
    "**5M9.** Redo {numref}`fig:piecewise` extending `x_max` from 6 to 12.\n",
    "\n",
    "1.  How this change affects the fit? \n",
    "\n",
    "2.  What are the implications for extrapolation? \n",
    "\n",
    "3.  add one more knot and make the necessary changes in the code so the     fit actually use the 3 knots.\n",
    "\n",
    "4.  change the position of the third new knot to improve the fit as much     as possible.\n",
    "\n",
    "**5M10.** For the bike rental example increase the number of knots. What is the effect on the fit? Change the width of the prior and visually evaluate the effect on the fit. What do you think the combination of knot number and prior weights controls? \n",
    "\n",
    "**5M11.** Fit the baby regression example from Chapter [4](chap3) using splines.\n",
    "\n",
    "**5M12.** In Code Block [bikes_dmatrix](bikes_dmatrix) we used a non-circular aware design matrix. Since we describe the hours in a day as cyclic, we want to use cyclic splines. However, there is one wrinkle. In the original dataset the hours range from 0 to 23, so using a circular spline patsy would treat 0 and 23 are the same. Still, we want a circular spline regression so perform the following steps.\n",
    "\n",
    "1.  Duplicate the 0 hour data label it as 24.\n",
    "\n",
    "2.  Generate a circular design matrix and a non-circular design matrix     with this modified dataset. Plot the results and compare.\n",
    "\n",
    "3.  Refit the bike spline dataset.\n",
    "\n",
    "4.  Explain what the effect of the circular spine regression was using     plots, numerical summaries, and diagnostics.\n",
    "\n",
    "**5M13.** For the rent bike example we use a Gaussian as likelihood, this can be seen as a reasonable approximation when the number of counts is large, but still brings some problems, like predicting negative number of rented bikes (for example, at night when the observed number of rented bikes is close to zero). To fix this issue and improve our models we can try with other likelihoods: \n",
    "\n",
    "1.  use a Poisson likelihood (hint you may need to restrict the $\\beta$     coefficients to be positive, and you can not normalize the data as     we did in the example). How the fit differs from the example in the     book. is this a better fit? In what sense? \n",
    "\n",
    "2.  use a NegativeBinomial likelihood, how the fit differs from the     previous two? Could you explain the differences (hint, the     NegativeBinomial can be considered as a mixture model of Poisson     distributions, which often helps to model overdispersed data) \n",
    "\n",
    "3.  Use LOO to compare the spline model with Poisson and     NegativeBinomial likelihoods. Which one has the best predictive     performance? \n",
    "\n",
    "4.  Can you justify the values of `p_loo` and the values of     $\\hat \\kappa$? \n",
    "\n",
    "5.  Use LOO-PIT to compare Gaussian, NegativeBinomial and Poisson models \n",
    "\n",
    "**5M14.** Using the model in Code Block [splines](splines) as a guide and for $X \\in [0, 1]$, set $\\tau \\sim \\text{Laplace}(0, 1)$: \n",
    "\n",
    "1.  Sample and plot realizations from the prior for $\\mu$. Use different     number and locations for the knots \n",
    "\n",
    "2.  What is the prior expectation for $\\mu(x_i)$ and how does it depend     on the knots and X? \n",
    "\n",
    "3.  What is the prior expectation for the standard deviations of     $\\mu(x_i)$ and how does it depend on the knots and X? \n",
    "\n",
    "4.  Repeat the previous points for the prior predictive distribution \n",
    "\n",
    "5.  Repeat the previous points using a $\\mathcal{H}\\text{C}(1)$ \n",
    "\n",
    "**5M15.** Fit the following data. Notice that the response variable is binary so you will need to adjust the likelihood accordingly and use a link function.\n",
    "\n",
    "1.  a logistic regression from a previous chapter. Visually compare the     results between both models.\n",
    "\n",
    "2.  Space Influenza is a disease which affects mostly young and old     people, but not middle-age folks. Fortunately, Space Influenza is     not a serious concern as it is completely made up. In this dataset     we have a record of people that got tested for Space Influenza and     whether they are sick (1) or healthy (0) and also their age. Could     you have solved this problem using logistic regression? \n",
    "\n",
    "**5M16.** Besides \"hour\" the bike dataset has other covariates, like \"temperature\". Fit a splines using both covariates. The simplest way to do this is by defining a separated spline/design matrix for each covariate. Fit a model with a NegativeBinomial likelihood.\n",
    "\n",
    "1.  Run diagnostics to check the sampling is correct and modify the     model and or sample hyperparameters accordingly.\n",
    "\n",
    "2.  How the rented bikes depend on the hours of the day and how on the     temperature? \n",
    "\n",
    "3.  Generate a model with only the hour covariate to the one with the     \"hour\" and \"temperature\". Compare both model using LOO, LOO-PIT and     posterior predictive checks.\n",
    "\n",
    "4.  Summarize all your findings \n",
    "\n",
    "---\n",
    "\n",
    "[^1]: See Runge's phenomenon for details. This can also be seen from     Taylor's theorem, polynomials will be useful to approximate a     function close to a single given point, but it will not be good over     its whole domain. If you got lost try watching this video     <https://www.youtube.com/watch?v=3d6DsjIBzJ4>.\n",
    "\n",
    "[^2]: A piecewise function is a function that is defined using     sub-functions, where each sub-function applies to a different     interval in the domain.\n",
    "\n",
    "[^3]: In Chapter [7](chap6) we explore how step-functions have a     central role in Bayesian Additive Regression Trees.\n",
    "\n",
    "[^4]: This can also be justified numerically as this reduces the number     of coefficients we need to find to compute a solution.\n",
    "\n",
    "[^5]: As usual the identity function is a valid choice.\n",
    "\n",
    "[^6]: Other basis functions could be wavelets or Fourier series as we     will see in Chapter [6](chap4).\n",
    "\n",
    "[^7]: Also known as break points, which is arguably a more memorable     name, but still knots is widely used in the literature.\n",
    "\n",
    "[^8]: In the limit of infinite degree a B-spline will span the entire     real line and not only that, it will converge to a Gaussian     <https://www.youtube.com/watch/9CS7j5I6aOc>.\n",
    "\n",
    "[^9]: Check     <https://pclambert.net/interactivegraphs/spline_continuity/spline_continuity>     for further intuition \n",
    "\n",
    "[^10]: If interested you can check     <https://en.wikipedia.org/wiki/De_Boor's_algorithm>.\n",
    "\n",
    " [^11]: <https://patsy.readthedocs.io> \n",
    "\n",
    "[^12]: <https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset> \n",
    "\n",
    "[^13]: Yes, this is also known as a mixed-effect model, you might recall     the related concept we discussed in Chapter [4](chap3)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
